{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "from function import clean_text, f1_m\n",
    "\n",
    "\n",
    "array_col = ['skincare', 'hair', 'make-up', 'other']\n",
    "\n",
    "### data\n",
    "df_train = pd.read_csv(\"data/hackathon_loreal_train_set.csv\")\n",
    "df_train = df_train.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "df_test = pd.read_csv(\"data/hackathon_loreal_test_set.csv\")\n",
    "df_test = df_test.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "df_train[\"clean_content\"] = df_train.text.apply(clean_text)\n",
    "df_test[\"clean_content\"] = df_test.text.apply(clean_text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['skeeterbytes',\n",
       " 'wrote',\n",
       " 'bassam',\n",
       " 'guy',\n",
       " 'wrote',\n",
       " \"n't\",\n",
       " 'anything',\n",
       " 'longer',\n",
       " '60mm',\n",
       " 'except',\n",
       " '99',\n",
       " 'plastic',\n",
       " 'fantastic',\n",
       " '``',\n",
       " '40-150',\n",
       " '.',\n",
       " \"'ve\",\n",
       " 'never',\n",
       " 'really',\n",
       " 'tried',\n",
       " 'avian',\n",
       " 'photography',\n",
       " 'would',\n",
       " 'like',\n",
       " 'get',\n",
       " 'decent',\n",
       " 'shot',\n",
       " 'unlike',\n",
       " 'opportunity',\n",
       " 'arises',\n",
       " 'used',\n",
       " 'c-af+tr',\n",
       " 'retrospect',\n",
       " 'since',\n",
       " 'turkey',\n",
       " 'lawyer',\n",
       " 'refused',\n",
       " 'fly',\n",
       " 'despite',\n",
       " 'plea',\n",
       " 'would',\n",
       " 's-af',\n",
       " 'done',\n",
       " 'better',\n",
       " 'mf',\n",
       " 'peaking',\n",
       " 'magnify',\n",
       " 'exposed',\n",
       " 'lawyer',\n",
       " 'dressed',\n",
       " 'dark',\n",
       " 'suit',\n",
       " 'bright',\n",
       " 'cloudy',\n",
       " 'sky',\n",
       " 'meteorologist',\n",
       " 'say',\n",
       " 'smoke',\n",
       " 'remnant',\n",
       " 'ca',\n",
       " 'northern',\n",
       " 'va',\n",
       " 'wa',\n",
       " '+2',\n",
       " '+2.5',\n",
       " 'ev',\n",
       " 'much',\n",
       " 'encouraged',\n",
       " 'chroma',\n",
       " \"'m\",\n",
       " 'great',\n",
       " 'judging',\n",
       " 'distance',\n",
       " 'would',\n",
       " 'say',\n",
       " 'wa',\n",
       " '35+',\n",
       " 'meter',\n",
       " 'away',\n",
       " 'pretty',\n",
       " 'intense',\n",
       " 'purple',\n",
       " 'fringing',\n",
       " 'lightroom',\n",
       " 'ha',\n",
       " 'effective',\n",
       " 'one-click',\n",
       " 'remover',\n",
       " \"'m\",\n",
       " 'sure',\n",
       " 'software',\n",
       " 'ha',\n",
       " 'hard',\n",
       " 'control',\n",
       " 'know',\n",
       " 'panasonic',\n",
       " '7-14',\n",
       " '4',\n",
       " 'never',\n",
       " 'use',\n",
       " 'c-af+tr',\n",
       " 'preferring',\n",
       " 'standard',\n",
       " 'c-af',\n",
       " 's-af',\n",
       " 'instance',\n",
       " 's-af',\n",
       " 'fine',\n",
       " 'attorney',\n",
       " \"n't\",\n",
       " 'going',\n",
       " 'anywhere',\n",
       " 'pay',\n",
       " 'bill',\n",
       " 'get',\n",
       " 's-af+magnify',\n",
       " 'allow',\n",
       " 'tweak',\n",
       " 'focus',\n",
       " 'plus',\n",
       " 'want',\n",
       " 'stop',\n",
       " 'lens',\n",
       " 'bit',\n",
       " 'sharpen',\n",
       " 'except',\n",
       " '7-14',\n",
       " 'nothing',\n",
       " 'else',\n",
       " 'f2.8',\n",
       " '5.6',\n",
       " 'seems',\n",
       " 'stopped',\n",
       " \"'ll\",\n",
       " 'watch',\n",
       " 'tried',\n",
       " 'magnify',\n",
       " 'wa',\n",
       " 'shaky',\n",
       " 'long',\n",
       " 'fl',\n",
       " \"'ll\",\n",
       " 'try',\n",
       " 'lower',\n",
       " 'scale',\n",
       " 'factor',\n",
       " 'also',\n",
       " 'raise',\n",
       " 'shutter',\n",
       " 'speed',\n",
       " 'freeze',\n",
       " 'flapping',\n",
       " \"'re\",\n",
       " 'iso200',\n",
       " 'good',\n",
       " 'lens',\n",
       " 'within',\n",
       " 'realm',\n",
       " '40-150',\n",
       " 'pro',\n",
       " 'league',\n",
       " 'better',\n",
       " '10x',\n",
       " 'price',\n",
       " 'birding',\n",
       " 'test',\n",
       " 'petapixel',\n",
       " 'fstoppers',\n",
       " 'rated',\n",
       " 'e-m5',\n",
       " 'iii',\n",
       " 'pretty',\n",
       " 'well',\n",
       " 'c-af+tr',\n",
       " \"n't\",\n",
       " 'use',\n",
       " 'often',\n",
       " 'location',\n",
       " '5',\n",
       " 'minute',\n",
       " 'walk',\n",
       " 'home',\n",
       " 'perhaps',\n",
       " \"'ll\",\n",
       " 'try',\n",
       " 'keep',\n",
       " 'experimenting',\n",
       " 'rick',\n",
       " 'thanks',\n",
       " 'helpful',\n",
       " 'reply']"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import re\n",
    "\n",
    "def delete_cara_no_useful(elem):\n",
    "    re.sub('[^a-zA-Z0-9 \\n\\.]', '',elem)\n",
    "    return elem\n",
    "\n",
    "tokenizeSingletext = word_tokenize(df_train.clean_content[0])\n",
    "result = list(map(delete_cara_no_useful, tokenizeSingletext))\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean = df_train.drop([\"text\"], axis = 1)\n",
    "df_test_clean = df_test.drop([\"text\"], axis = 1)\n",
    "\n",
    "#X = df_train['clean_content']\n",
    "#Y = df_train.drop(['clean_content', 'index', 'text'], axis = 1)\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=4)\n",
    "\n",
    "x_train = df_train_clean[\"clean_content\"]\n",
    "x_test = df_test_clean[\"clean_content\"]\n",
    "\n",
    "y_train = df_train_clean.drop([\"clean_content\", \"index\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10572,)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "331/331 [==============================] - 22s 65ms/step - loss: 0.2583 - f1_m: 0.7886\n",
      "Epoch 2/5\n",
      "331/331 [==============================] - 20s 60ms/step - loss: 0.1142 - f1_m: 0.9238\n",
      "Epoch 3/5\n",
      "331/331 [==============================] - 28s 85ms/step - loss: 0.0661 - f1_m: 0.9575\n",
      "Epoch 4/5\n",
      "331/331 [==============================] - 32s 98ms/step - loss: 0.0511 - f1_m: 0.9647\n",
      "Epoch 5/5\n",
      "331/331 [==============================] - 24s 74ms/step - loss: 0.0450 - f1_m: 0.9680\n",
      "83/83 [==============================] - 2s 24ms/step - loss: 32685.2734 - f1_m: 2.5110\n"
     ]
    }
   ],
   "source": [
    "cv=CountVectorizer()\n",
    "cv.fit(x_train)\n",
    "\n",
    "x_train_count = cv.fit_transform(x_train).astype('int32').toarray()\n",
    "x_test_count = cv.transform(x_test).astype('int32').toarray()\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_shape=(80736,), activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = [f1_m])\n",
    "\n",
    "model.fit(x_train_count, y_train, epochs=5)\n",
    "\n",
    "\n",
    "evaluate = model.evaluate(x_test_count, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}