{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       Unnamed: 0                                               text  \\\n",
       "0               0  Skeeterbytes wrote: Bassam Guy wrote: I don't ...   \n",
       "1               1  Refueling before the long week üçµ - You‚Äôre mean...   \n",
       "2               2  Just in case no one has told you today YOU ARE...   \n",
       "3               3  Do not forget to get some sunshine safely if y...   \n",
       "4               4  Abstract Background Humans have dramatically c...   \n",
       "...           ...                                                ...   \n",
       "10567       13209  ‚òë Day: 273 Outifit: Thor (Gold Foil) Back Blin...   \n",
       "10568       13210  In order to study the various trends and patte...   \n",
       "10569       13212  This brand has not yet registered with Influen...   \n",
       "10570       13213  here is my regime get rid of any acne scars an...   \n",
       "10571       13214  It's true, I used to work as a caddie at Trump...   \n",
       "\n",
       "       skincare  hair  make-up  other  index  \n",
       "0           0.0   0.0      0.0    1.0      0  \n",
       "1           0.0   0.0      0.0    1.0      1  \n",
       "2           0.0   0.0      1.0    0.0      2  \n",
       "3           0.0   0.0      0.0    1.0      3  \n",
       "4           0.0   0.0      0.0    1.0      4  \n",
       "...         ...   ...      ...    ...    ...  \n",
       "10567       0.0   0.0      0.0    1.0  13209  \n",
       "10568       0.0   0.0      0.0    1.0  13210  \n",
       "10569       0.0   0.0      0.0    1.0  13212  \n",
       "10570       1.0   0.0      0.0    0.0  13213  \n",
       "10571       0.0   0.0      0.0    1.0  13214  \n",
       "\n",
       "[10572 rows x 7 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>text</th>\n      <th>skincare</th>\n      <th>hair</th>\n      <th>make-up</th>\n      <th>other</th>\n      <th>index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Skeeterbytes wrote: Bassam Guy wrote: I don't ...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Refueling before the long week üçµ - You‚Äôre mean...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Just in case no one has told you today YOU ARE...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Do not forget to get some sunshine safely if y...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Abstract Background Humans have dramatically c...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10567</th>\n      <td>13209</td>\n      <td>‚òë Day: 273 Outifit: Thor (Gold Foil) Back Blin...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>13209</td>\n    </tr>\n    <tr>\n      <th>10568</th>\n      <td>13210</td>\n      <td>In order to study the various trends and patte...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>13210</td>\n    </tr>\n    <tr>\n      <th>10569</th>\n      <td>13212</td>\n      <td>This brand has not yet registered with Influen...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>13212</td>\n    </tr>\n    <tr>\n      <th>10570</th>\n      <td>13213</td>\n      <td>here is my regime get rid of any acne scars an...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>13213</td>\n    </tr>\n    <tr>\n      <th>10571</th>\n      <td>13214</td>\n      <td>It's true, I used to work as a caddie at Trump...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>13214</td>\n    </tr>\n  </tbody>\n</table>\n<p>10572 rows √ó 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"data/hackathon_loreal_train_set.csv\")\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# watch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'text', 'skincare', 'hair', 'make-up', 'other', 'index'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, text, skincare, hair, make-up, other, index]\n",
       "Index: []"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>text</th>\n      <th>skincare</th>\n      <th>hair</th>\n      <th>make-up</th>\n      <th>other</th>\n      <th>index</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df = df_train[df_train['Unnamed: 0'] != df_train['index']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# delete one columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    text  skincare  hair  \\\n",
       "0      Skeeterbytes wrote: Bassam Guy wrote: I don't ...       0.0   0.0   \n",
       "1      Refueling before the long week üçµ - You‚Äôre mean...       0.0   0.0   \n",
       "2      Just in case no one has told you today YOU ARE...       0.0   0.0   \n",
       "3      Do not forget to get some sunshine safely if y...       0.0   0.0   \n",
       "4      Abstract Background Humans have dramatically c...       0.0   0.0   \n",
       "...                                                  ...       ...   ...   \n",
       "10567  ‚òë Day: 273 Outifit: Thor (Gold Foil) Back Blin...       0.0   0.0   \n",
       "10568  In order to study the various trends and patte...       0.0   0.0   \n",
       "10569  This brand has not yet registered with Influen...       0.0   0.0   \n",
       "10570  here is my regime get rid of any acne scars an...       1.0   0.0   \n",
       "10571  It's true, I used to work as a caddie at Trump...       0.0   0.0   \n",
       "\n",
       "       make-up  other  index  \n",
       "0          0.0    1.0      0  \n",
       "1          0.0    1.0      1  \n",
       "2          1.0    0.0      2  \n",
       "3          0.0    1.0      3  \n",
       "4          0.0    1.0      4  \n",
       "...        ...    ...    ...  \n",
       "10567      0.0    1.0  13209  \n",
       "10568      0.0    1.0  13210  \n",
       "10569      0.0    1.0  13212  \n",
       "10570      0.0    0.0  13213  \n",
       "10571      0.0    1.0  13214  \n",
       "\n",
       "[10572 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>skincare</th>\n      <th>hair</th>\n      <th>make-up</th>\n      <th>other</th>\n      <th>index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Skeeterbytes wrote: Bassam Guy wrote: I don't ...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Refueling before the long week üçµ - You‚Äôre mean...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Just in case no one has told you today YOU ARE...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Do not forget to get some sunshine safely if y...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Abstract Background Humans have dramatically c...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10567</th>\n      <td>‚òë Day: 273 Outifit: Thor (Gold Foil) Back Blin...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>13209</td>\n    </tr>\n    <tr>\n      <th>10568</th>\n      <td>In order to study the various trends and patte...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>13210</td>\n    </tr>\n    <tr>\n      <th>10569</th>\n      <td>This brand has not yet registered with Influen...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>13212</td>\n    </tr>\n    <tr>\n      <th>10570</th>\n      <td>here is my regime get rid of any acne scars an...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>13213</td>\n    </tr>\n    <tr>\n      <th>10571</th>\n      <td>It's true, I used to work as a caddie at Trump...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>13214</td>\n    </tr>\n  </tbody>\n</table>\n<p>10572 rows √ó 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df_train = df_train.drop(['Unnamed: 0'], axis = 1)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing one text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from nltk import word_tokenize, WordNetLemmatizer, PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk import ngrams\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Refueling before the long week üçµ - You‚Äôre meant to Drink it, but I prefer it in my üçµ From to , matcha is a natural powder of freshly ground leaves! ‚úÖthanks to its numerous antioxidant benefits! ‚úÖIncluding it in your beauty routine requires very little time and effort to see benefits ‚úÖ The best part of it is that it‚Äôs already in your cupboard and doubles as a delicious drink? It really doesn‚Äôt get better than this! üõí Matcha tea purchased from my favorite online store faithfultonature'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "Onetext = df_train['text'].iloc[1]\n",
    "Onetext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to remove emoji.\n",
    "def emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Refueling before the long week  - You‚Äôre meant to Drink it, but I prefer it in my  From to , matcha is a natural powder of freshly ground leaves! thanks to its numerous antioxidant benefits! Including it in your beauty routine requires very little time and effort to see benefits  The best part of it is that it‚Äôs already in your cupboard and doubles as a delicious drink? It really doesn‚Äôt get better than this!  Matcha tea purchased from my favorite online store faithfultonature'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "emoji(Onetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'skeeterbytes wrote: bassam guy wrote: i don\\'t have anything longer than 60mm except my $99 \"plastic [not so] fantastic\" 40-150. i\\'ve never really tried avian photography before but would like to get some decent shots (unlike these) when the opportunity arises. i used c-af+tr. in retrospect, since the turkey lawyer refused to fly despite my pleas. would s-af have done better? mf, peaking or magnify or both? i exposed for the lawyer, dressed in dark suits against a bright cloudy sky (some meteorologists say it is smoke remnants from ca in northern va). was +2 and +2.5 ev too much and encouraged the chroma? i\\'m not great at judging distance but i would say i was about 35+ meters away. pretty intense purple fringing. lightroom has an effective one-click remover and i\\'m sure other software has, too. that should not be hard to control. i know how to do that. i own the panasonic 7-14 4 never use c-af+tr myself, preferring standard c-af or s-af. in this instance, s-af is fine because your attorney isn\\'t going anywhere until you pay your bill [get it?] and s-af+magnify will allow you to tweak focus. plus, you want to stop down the lens a bit and it will sharpen; except for the 7-14, i have nothing else above f2.8. 5.6 seems stopped down. i\\'ll watch it. i tried magnify but it was too shaky at the long fl. i\\'ll try a lower scale factor. also, raise shutter speed to freeze the flapping about (you\\'re below iso200). it\\'s a good lens within its realm but the 40-150 pro is leagues better. as it should be at 10x the price. some birding test from petapixel or fstoppers or such rated the e-m5 iii pretty well for c-af+tr. i don\\'t use it often. that location is a 5 minute walk from my home. perhaps i\\'ll try again. keep experimenting! rick thanks for your helpful reply.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OnetextLower = Onetext.lower()\n",
    "OnetextLower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['skeeterbytes',\n",
       " 'wrote',\n",
       " ':',\n",
       " 'bassam',\n",
       " 'guy',\n",
       " 'wrote',\n",
       " ':',\n",
       " 'i',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'have',\n",
       " 'anything',\n",
       " 'longer',\n",
       " 'than',\n",
       " '60mm',\n",
       " 'except',\n",
       " 'my',\n",
       " '$',\n",
       " '99',\n",
       " '``',\n",
       " 'plastic',\n",
       " '[',\n",
       " 'not',\n",
       " 'so',\n",
       " ']',\n",
       " 'fantastic',\n",
       " \"''\",\n",
       " '40-150.',\n",
       " 'i',\n",
       " \"'ve\",\n",
       " 'never',\n",
       " 'really',\n",
       " 'tried',\n",
       " 'avian',\n",
       " 'photography',\n",
       " 'before',\n",
       " 'but',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'get',\n",
       " 'some',\n",
       " 'decent',\n",
       " 'shots',\n",
       " '(',\n",
       " 'unlike',\n",
       " 'these',\n",
       " ')',\n",
       " 'when',\n",
       " 'the',\n",
       " 'opportunity',\n",
       " 'arises',\n",
       " '.',\n",
       " 'i',\n",
       " 'used',\n",
       " 'c-af+tr',\n",
       " '.',\n",
       " 'in',\n",
       " 'retrospect',\n",
       " ',',\n",
       " 'since',\n",
       " 'the',\n",
       " 'turkey',\n",
       " 'lawyer',\n",
       " 'refused',\n",
       " 'to',\n",
       " 'fly',\n",
       " 'despite',\n",
       " 'my',\n",
       " 'pleas',\n",
       " '.',\n",
       " 'would',\n",
       " 's-af',\n",
       " 'have',\n",
       " 'done',\n",
       " 'better',\n",
       " '?',\n",
       " 'mf',\n",
       " ',',\n",
       " 'peaking',\n",
       " 'or',\n",
       " 'magnify',\n",
       " 'or',\n",
       " 'both',\n",
       " '?',\n",
       " 'i',\n",
       " 'exposed',\n",
       " 'for',\n",
       " 'the',\n",
       " 'lawyer',\n",
       " ',',\n",
       " 'dressed',\n",
       " 'in',\n",
       " 'dark',\n",
       " 'suits',\n",
       " 'against',\n",
       " 'a',\n",
       " 'bright',\n",
       " 'cloudy',\n",
       " 'sky',\n",
       " '(',\n",
       " 'some',\n",
       " 'meteorologists',\n",
       " 'say',\n",
       " 'it',\n",
       " 'is',\n",
       " 'smoke',\n",
       " 'remnants',\n",
       " 'from',\n",
       " 'ca',\n",
       " 'in',\n",
       " 'northern',\n",
       " 'va',\n",
       " ')',\n",
       " '.',\n",
       " 'was',\n",
       " '+2',\n",
       " 'and',\n",
       " '+2.5',\n",
       " 'ev',\n",
       " 'too',\n",
       " 'much',\n",
       " 'and',\n",
       " 'encouraged',\n",
       " 'the',\n",
       " 'chroma',\n",
       " '?',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'not',\n",
       " 'great',\n",
       " 'at',\n",
       " 'judging',\n",
       " 'distance',\n",
       " 'but',\n",
       " 'i',\n",
       " 'would',\n",
       " 'say',\n",
       " 'i',\n",
       " 'was',\n",
       " 'about',\n",
       " '35+',\n",
       " 'meters',\n",
       " 'away',\n",
       " '.',\n",
       " 'pretty',\n",
       " 'intense',\n",
       " 'purple',\n",
       " 'fringing',\n",
       " '.',\n",
       " 'lightroom',\n",
       " 'has',\n",
       " 'an',\n",
       " 'effective',\n",
       " 'one-click',\n",
       " 'remover',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'sure',\n",
       " 'other',\n",
       " 'software',\n",
       " 'has',\n",
       " ',',\n",
       " 'too',\n",
       " '.',\n",
       " 'that',\n",
       " 'should',\n",
       " 'not',\n",
       " 'be',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'control',\n",
       " '.',\n",
       " 'i',\n",
       " 'know',\n",
       " 'how',\n",
       " 'to',\n",
       " 'do',\n",
       " 'that',\n",
       " '.',\n",
       " 'i',\n",
       " 'own',\n",
       " 'the',\n",
       " 'panasonic',\n",
       " '7-14',\n",
       " '4',\n",
       " 'never',\n",
       " 'use',\n",
       " 'c-af+tr',\n",
       " 'myself',\n",
       " ',',\n",
       " 'preferring',\n",
       " 'standard',\n",
       " 'c-af',\n",
       " 'or',\n",
       " 's-af',\n",
       " '.',\n",
       " 'in',\n",
       " 'this',\n",
       " 'instance',\n",
       " ',',\n",
       " 's-af',\n",
       " 'is',\n",
       " 'fine',\n",
       " 'because',\n",
       " 'your',\n",
       " 'attorney',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'going',\n",
       " 'anywhere',\n",
       " 'until',\n",
       " 'you',\n",
       " 'pay',\n",
       " 'your',\n",
       " 'bill',\n",
       " '[',\n",
       " 'get',\n",
       " 'it',\n",
       " '?',\n",
       " ']',\n",
       " 'and',\n",
       " 's-af+magnify',\n",
       " 'will',\n",
       " 'allow',\n",
       " 'you',\n",
       " 'to',\n",
       " 'tweak',\n",
       " 'focus',\n",
       " '.',\n",
       " 'plus',\n",
       " ',',\n",
       " 'you',\n",
       " 'want',\n",
       " 'to',\n",
       " 'stop',\n",
       " 'down',\n",
       " 'the',\n",
       " 'lens',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'and',\n",
       " 'it',\n",
       " 'will',\n",
       " 'sharpen',\n",
       " ';',\n",
       " 'except',\n",
       " 'for',\n",
       " 'the',\n",
       " '7-14',\n",
       " ',',\n",
       " 'i',\n",
       " 'have',\n",
       " 'nothing',\n",
       " 'else',\n",
       " 'above',\n",
       " 'f2.8',\n",
       " '.',\n",
       " '5.6',\n",
       " 'seems',\n",
       " 'stopped',\n",
       " 'down',\n",
       " '.',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'watch',\n",
       " 'it',\n",
       " '.',\n",
       " 'i',\n",
       " 'tried',\n",
       " 'magnify',\n",
       " 'but',\n",
       " 'it',\n",
       " 'was',\n",
       " 'too',\n",
       " 'shaky',\n",
       " 'at',\n",
       " 'the',\n",
       " 'long',\n",
       " 'fl',\n",
       " '.',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'try',\n",
       " 'a',\n",
       " 'lower',\n",
       " 'scale',\n",
       " 'factor',\n",
       " '.',\n",
       " 'also',\n",
       " ',',\n",
       " 'raise',\n",
       " 'shutter',\n",
       " 'speed',\n",
       " 'to',\n",
       " 'freeze',\n",
       " 'the',\n",
       " 'flapping',\n",
       " 'about',\n",
       " '(',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'below',\n",
       " 'iso200',\n",
       " ')',\n",
       " '.',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'good',\n",
       " 'lens',\n",
       " 'within',\n",
       " 'its',\n",
       " 'realm',\n",
       " 'but',\n",
       " 'the',\n",
       " '40-150',\n",
       " 'pro',\n",
       " 'is',\n",
       " 'leagues',\n",
       " 'better',\n",
       " '.',\n",
       " 'as',\n",
       " 'it',\n",
       " 'should',\n",
       " 'be',\n",
       " 'at',\n",
       " '10x',\n",
       " 'the',\n",
       " 'price',\n",
       " '.',\n",
       " 'some',\n",
       " 'birding',\n",
       " 'test',\n",
       " 'from',\n",
       " 'petapixel',\n",
       " 'or',\n",
       " 'fstoppers',\n",
       " 'or',\n",
       " 'such',\n",
       " 'rated',\n",
       " 'the',\n",
       " 'e-m5',\n",
       " 'iii',\n",
       " 'pretty',\n",
       " 'well',\n",
       " 'for',\n",
       " 'c-af+tr',\n",
       " '.',\n",
       " 'i',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'use',\n",
       " 'it',\n",
       " 'often',\n",
       " '.',\n",
       " 'that',\n",
       " 'location',\n",
       " 'is',\n",
       " 'a',\n",
       " '5',\n",
       " 'minute',\n",
       " 'walk',\n",
       " 'from',\n",
       " 'my',\n",
       " 'home',\n",
       " '.',\n",
       " 'perhaps',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'try',\n",
       " 'again',\n",
       " '.',\n",
       " 'keep',\n",
       " 'experimenting',\n",
       " '!',\n",
       " 'rick',\n",
       " 'thanks',\n",
       " 'for',\n",
       " 'your',\n",
       " 'helpful',\n",
       " 'reply',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizeSingletext = word_tokenize(OnetextLower)\n",
    "tokenizeSingletext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['skeeterbytes',\n",
       " 'wrote',\n",
       " ':',\n",
       " 'bassam',\n",
       " 'guy',\n",
       " 'wrote',\n",
       " ':',\n",
       " 'i',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'have',\n",
       " 'anything',\n",
       " 'longer',\n",
       " 'than',\n",
       " '60mm',\n",
       " 'except',\n",
       " 'my',\n",
       " '$',\n",
       " '99',\n",
       " '``',\n",
       " 'plastic',\n",
       " '[',\n",
       " 'not',\n",
       " 'so',\n",
       " ']',\n",
       " 'fantastic',\n",
       " \"''\",\n",
       " '40-150.',\n",
       " 'i',\n",
       " \"'ve\",\n",
       " 'never',\n",
       " 'really',\n",
       " 'tried',\n",
       " 'avian',\n",
       " 'photography',\n",
       " 'before',\n",
       " 'but',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'get',\n",
       " 'some',\n",
       " 'decent',\n",
       " 'shot',\n",
       " '(',\n",
       " 'unlike',\n",
       " 'these',\n",
       " ')',\n",
       " 'when',\n",
       " 'the',\n",
       " 'opportunity',\n",
       " 'arises',\n",
       " '.',\n",
       " 'i',\n",
       " 'used',\n",
       " 'c-af+tr',\n",
       " '.',\n",
       " 'in',\n",
       " 'retrospect',\n",
       " ',',\n",
       " 'since',\n",
       " 'the',\n",
       " 'turkey',\n",
       " 'lawyer',\n",
       " 'refused',\n",
       " 'to',\n",
       " 'fly',\n",
       " 'despite',\n",
       " 'my',\n",
       " 'plea',\n",
       " '.',\n",
       " 'would',\n",
       " 's-af',\n",
       " 'have',\n",
       " 'done',\n",
       " 'better',\n",
       " '?',\n",
       " 'mf',\n",
       " ',',\n",
       " 'peaking',\n",
       " 'or',\n",
       " 'magnify',\n",
       " 'or',\n",
       " 'both',\n",
       " '?',\n",
       " 'i',\n",
       " 'exposed',\n",
       " 'for',\n",
       " 'the',\n",
       " 'lawyer',\n",
       " ',',\n",
       " 'dressed',\n",
       " 'in',\n",
       " 'dark',\n",
       " 'suit',\n",
       " 'against',\n",
       " 'a',\n",
       " 'bright',\n",
       " 'cloudy',\n",
       " 'sky',\n",
       " '(',\n",
       " 'some',\n",
       " 'meteorologist',\n",
       " 'say',\n",
       " 'it',\n",
       " 'is',\n",
       " 'smoke',\n",
       " 'remnant',\n",
       " 'from',\n",
       " 'ca',\n",
       " 'in',\n",
       " 'northern',\n",
       " 'va',\n",
       " ')',\n",
       " '.',\n",
       " 'wa',\n",
       " '+2',\n",
       " 'and',\n",
       " '+2.5',\n",
       " 'ev',\n",
       " 'too',\n",
       " 'much',\n",
       " 'and',\n",
       " 'encouraged',\n",
       " 'the',\n",
       " 'chroma',\n",
       " '?',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'not',\n",
       " 'great',\n",
       " 'at',\n",
       " 'judging',\n",
       " 'distance',\n",
       " 'but',\n",
       " 'i',\n",
       " 'would',\n",
       " 'say',\n",
       " 'i',\n",
       " 'wa',\n",
       " 'about',\n",
       " '35+',\n",
       " 'meter',\n",
       " 'away',\n",
       " '.',\n",
       " 'pretty',\n",
       " 'intense',\n",
       " 'purple',\n",
       " 'fringing',\n",
       " '.',\n",
       " 'lightroom',\n",
       " 'ha',\n",
       " 'an',\n",
       " 'effective',\n",
       " 'one-click',\n",
       " 'remover',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'sure',\n",
       " 'other',\n",
       " 'software',\n",
       " 'ha',\n",
       " ',',\n",
       " 'too',\n",
       " '.',\n",
       " 'that',\n",
       " 'should',\n",
       " 'not',\n",
       " 'be',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'control',\n",
       " '.',\n",
       " 'i',\n",
       " 'know',\n",
       " 'how',\n",
       " 'to',\n",
       " 'do',\n",
       " 'that',\n",
       " '.',\n",
       " 'i',\n",
       " 'own',\n",
       " 'the',\n",
       " 'panasonic',\n",
       " '7-14',\n",
       " '4',\n",
       " 'never',\n",
       " 'use',\n",
       " 'c-af+tr',\n",
       " 'myself',\n",
       " ',',\n",
       " 'preferring',\n",
       " 'standard',\n",
       " 'c-af',\n",
       " 'or',\n",
       " 's-af',\n",
       " '.',\n",
       " 'in',\n",
       " 'this',\n",
       " 'instance',\n",
       " ',',\n",
       " 's-af',\n",
       " 'is',\n",
       " 'fine',\n",
       " 'because',\n",
       " 'your',\n",
       " 'attorney',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'going',\n",
       " 'anywhere',\n",
       " 'until',\n",
       " 'you',\n",
       " 'pay',\n",
       " 'your',\n",
       " 'bill',\n",
       " '[',\n",
       " 'get',\n",
       " 'it',\n",
       " '?',\n",
       " ']',\n",
       " 'and',\n",
       " 's-af+magnify',\n",
       " 'will',\n",
       " 'allow',\n",
       " 'you',\n",
       " 'to',\n",
       " 'tweak',\n",
       " 'focus',\n",
       " '.',\n",
       " 'plus',\n",
       " ',',\n",
       " 'you',\n",
       " 'want',\n",
       " 'to',\n",
       " 'stop',\n",
       " 'down',\n",
       " 'the',\n",
       " 'lens',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'and',\n",
       " 'it',\n",
       " 'will',\n",
       " 'sharpen',\n",
       " ';',\n",
       " 'except',\n",
       " 'for',\n",
       " 'the',\n",
       " '7-14',\n",
       " ',',\n",
       " 'i',\n",
       " 'have',\n",
       " 'nothing',\n",
       " 'else',\n",
       " 'above',\n",
       " 'f2.8',\n",
       " '.',\n",
       " '5.6',\n",
       " 'seems',\n",
       " 'stopped',\n",
       " 'down',\n",
       " '.',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'watch',\n",
       " 'it',\n",
       " '.',\n",
       " 'i',\n",
       " 'tried',\n",
       " 'magnify',\n",
       " 'but',\n",
       " 'it',\n",
       " 'wa',\n",
       " 'too',\n",
       " 'shaky',\n",
       " 'at',\n",
       " 'the',\n",
       " 'long',\n",
       " 'fl',\n",
       " '.',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'try',\n",
       " 'a',\n",
       " 'lower',\n",
       " 'scale',\n",
       " 'factor',\n",
       " '.',\n",
       " 'also',\n",
       " ',',\n",
       " 'raise',\n",
       " 'shutter',\n",
       " 'speed',\n",
       " 'to',\n",
       " 'freeze',\n",
       " 'the',\n",
       " 'flapping',\n",
       " 'about',\n",
       " '(',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'below',\n",
       " 'iso200',\n",
       " ')',\n",
       " '.',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'good',\n",
       " 'lens',\n",
       " 'within',\n",
       " 'it',\n",
       " 'realm',\n",
       " 'but',\n",
       " 'the',\n",
       " '40-150',\n",
       " 'pro',\n",
       " 'is',\n",
       " 'league',\n",
       " 'better',\n",
       " '.',\n",
       " 'a',\n",
       " 'it',\n",
       " 'should',\n",
       " 'be',\n",
       " 'at',\n",
       " '10x',\n",
       " 'the',\n",
       " 'price',\n",
       " '.',\n",
       " 'some',\n",
       " 'birding',\n",
       " 'test',\n",
       " 'from',\n",
       " 'petapixel',\n",
       " 'or',\n",
       " 'fstoppers',\n",
       " 'or',\n",
       " 'such',\n",
       " 'rated',\n",
       " 'the',\n",
       " 'e-m5',\n",
       " 'iii',\n",
       " 'pretty',\n",
       " 'well',\n",
       " 'for',\n",
       " 'c-af+tr',\n",
       " '.',\n",
       " 'i',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'use',\n",
       " 'it',\n",
       " 'often',\n",
       " '.',\n",
       " 'that',\n",
       " 'location',\n",
       " 'is',\n",
       " 'a',\n",
       " '5',\n",
       " 'minute',\n",
       " 'walk',\n",
       " 'from',\n",
       " 'my',\n",
       " 'home',\n",
       " '.',\n",
       " 'perhaps',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'try',\n",
       " 'again',\n",
       " '.',\n",
       " 'keep',\n",
       " 'experimenting',\n",
       " '!',\n",
       " 'rick',\n",
       " 'thanks',\n",
       " 'for',\n",
       " 'your',\n",
       " 'helpful',\n",
       " 'reply',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizeSingleText = list(map(lemmatizer.lemmatize, tokenizeSingletext))\n",
    "lemmatizeSingleText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['skeeterbytes',\n",
       " 'wrote',\n",
       " 'bassam',\n",
       " 'guy',\n",
       " 'wrote',\n",
       " 'i',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'have',\n",
       " 'anything',\n",
       " 'longer',\n",
       " 'than',\n",
       " '60mm',\n",
       " 'except',\n",
       " 'my',\n",
       " '$',\n",
       " '99',\n",
       " 'plastic',\n",
       " 'not',\n",
       " 'so',\n",
       " 'fantastic',\n",
       " \"''\",\n",
       " '40-150.',\n",
       " 'i',\n",
       " \"'ve\",\n",
       " 'never',\n",
       " 'really',\n",
       " 'tried',\n",
       " 'avian',\n",
       " 'photography',\n",
       " 'before',\n",
       " 'but',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'get',\n",
       " 'some',\n",
       " 'decent',\n",
       " 'shot',\n",
       " 'unlike',\n",
       " 'these',\n",
       " 'when',\n",
       " 'the',\n",
       " 'opportunity',\n",
       " 'arises',\n",
       " 'i',\n",
       " 'used',\n",
       " 'c-af+tr',\n",
       " 'in',\n",
       " 'retrospect',\n",
       " 'since',\n",
       " 'the',\n",
       " 'turkey',\n",
       " 'lawyer',\n",
       " 'refused',\n",
       " 'to',\n",
       " 'fly',\n",
       " 'despite',\n",
       " 'my',\n",
       " 'plea',\n",
       " 'would',\n",
       " 's-af',\n",
       " 'have',\n",
       " 'done',\n",
       " 'better',\n",
       " 'mf',\n",
       " 'peaking',\n",
       " 'or',\n",
       " 'magnify',\n",
       " 'or',\n",
       " 'both',\n",
       " 'i',\n",
       " 'exposed',\n",
       " 'for',\n",
       " 'the',\n",
       " 'lawyer',\n",
       " 'dressed',\n",
       " 'in',\n",
       " 'dark',\n",
       " 'suit',\n",
       " 'against',\n",
       " 'a',\n",
       " 'bright',\n",
       " 'cloudy',\n",
       " 'sky',\n",
       " 'some',\n",
       " 'meteorologist',\n",
       " 'say',\n",
       " 'it',\n",
       " 'is',\n",
       " 'smoke',\n",
       " 'remnant',\n",
       " 'from',\n",
       " 'ca',\n",
       " 'in',\n",
       " 'northern',\n",
       " 'va',\n",
       " 'wa',\n",
       " '+2',\n",
       " 'and',\n",
       " '+2.5',\n",
       " 'ev',\n",
       " 'too',\n",
       " 'much',\n",
       " 'and',\n",
       " 'encouraged',\n",
       " 'the',\n",
       " 'chroma',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'not',\n",
       " 'great',\n",
       " 'at',\n",
       " 'judging',\n",
       " 'distance',\n",
       " 'but',\n",
       " 'i',\n",
       " 'would',\n",
       " 'say',\n",
       " 'i',\n",
       " 'wa',\n",
       " 'about',\n",
       " '35+',\n",
       " 'meter',\n",
       " 'away',\n",
       " 'pretty',\n",
       " 'intense',\n",
       " 'purple',\n",
       " 'fringing',\n",
       " 'lightroom',\n",
       " 'ha',\n",
       " 'an',\n",
       " 'effective',\n",
       " 'one-click',\n",
       " 'remover',\n",
       " 'and',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'sure',\n",
       " 'other',\n",
       " 'software',\n",
       " 'ha',\n",
       " 'too',\n",
       " 'that',\n",
       " 'should',\n",
       " 'not',\n",
       " 'be',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'control',\n",
       " 'i',\n",
       " 'know',\n",
       " 'how',\n",
       " 'to',\n",
       " 'do',\n",
       " 'that',\n",
       " 'i',\n",
       " 'own',\n",
       " 'the',\n",
       " 'panasonic',\n",
       " '7-14',\n",
       " '4',\n",
       " 'never',\n",
       " 'use',\n",
       " 'c-af+tr',\n",
       " 'myself',\n",
       " 'preferring',\n",
       " 'standard',\n",
       " 'c-af',\n",
       " 'or',\n",
       " 's-af',\n",
       " 'in',\n",
       " 'this',\n",
       " 'instance',\n",
       " 's-af',\n",
       " 'is',\n",
       " 'fine',\n",
       " 'because',\n",
       " 'your',\n",
       " 'attorney',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'going',\n",
       " 'anywhere',\n",
       " 'until',\n",
       " 'you',\n",
       " 'pay',\n",
       " 'your',\n",
       " 'bill',\n",
       " 'get',\n",
       " 'it',\n",
       " 'and',\n",
       " 's-af+magnify',\n",
       " 'will',\n",
       " 'allow',\n",
       " 'you',\n",
       " 'to',\n",
       " 'tweak',\n",
       " 'focus',\n",
       " 'plus',\n",
       " 'you',\n",
       " 'want',\n",
       " 'to',\n",
       " 'stop',\n",
       " 'down',\n",
       " 'the',\n",
       " 'lens',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'and',\n",
       " 'it',\n",
       " 'will',\n",
       " 'sharpen',\n",
       " 'except',\n",
       " 'for',\n",
       " 'the',\n",
       " '7-14',\n",
       " 'i',\n",
       " 'have',\n",
       " 'nothing',\n",
       " 'else',\n",
       " 'above',\n",
       " 'f2.8',\n",
       " '5.6',\n",
       " 'seems',\n",
       " 'stopped',\n",
       " 'down',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'watch',\n",
       " 'it',\n",
       " 'i',\n",
       " 'tried',\n",
       " 'magnify',\n",
       " 'but',\n",
       " 'it',\n",
       " 'wa',\n",
       " 'too',\n",
       " 'shaky',\n",
       " 'at',\n",
       " 'the',\n",
       " 'long',\n",
       " 'fl',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'try',\n",
       " 'a',\n",
       " 'lower',\n",
       " 'scale',\n",
       " 'factor',\n",
       " 'also',\n",
       " 'raise',\n",
       " 'shutter',\n",
       " 'speed',\n",
       " 'to',\n",
       " 'freeze',\n",
       " 'the',\n",
       " 'flapping',\n",
       " 'about',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'below',\n",
       " 'iso200',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'good',\n",
       " 'lens',\n",
       " 'within',\n",
       " 'it',\n",
       " 'realm',\n",
       " 'but',\n",
       " 'the',\n",
       " '40-150',\n",
       " 'pro',\n",
       " 'is',\n",
       " 'league',\n",
       " 'better',\n",
       " 'a',\n",
       " 'it',\n",
       " 'should',\n",
       " 'be',\n",
       " 'at',\n",
       " '10x',\n",
       " 'the',\n",
       " 'price',\n",
       " 'some',\n",
       " 'birding',\n",
       " 'test',\n",
       " 'from',\n",
       " 'petapixel',\n",
       " 'or',\n",
       " 'fstoppers',\n",
       " 'or',\n",
       " 'such',\n",
       " 'rated',\n",
       " 'the',\n",
       " 'e-m5',\n",
       " 'iii',\n",
       " 'pretty',\n",
       " 'well',\n",
       " 'for',\n",
       " 'c-af+tr',\n",
       " 'i',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'use',\n",
       " 'it',\n",
       " 'often',\n",
       " 'that',\n",
       " 'location',\n",
       " 'is',\n",
       " 'a',\n",
       " '5',\n",
       " 'minute',\n",
       " 'walk',\n",
       " 'from',\n",
       " 'my',\n",
       " 'home',\n",
       " 'perhaps',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'try',\n",
       " 'again',\n",
       " 'keep',\n",
       " 'experimenting',\n",
       " 'rick',\n",
       " 'thanks',\n",
       " 'for',\n",
       " 'your',\n",
       " 'helpful',\n",
       " 'reply']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_punctuation = [':', '(', ')', '/', '|', ',', ']', ';',\n",
    "                    '.', '*', '#', '\"', '&', '~', '``',\n",
    "                    '-', '_', '\\\\', '@','?','!','\\'', '[']\n",
    "TextCleanPonctuation = [word for word in lemmatizeSingleText if word not in stop_punctuation]\n",
    "TextCleanPonctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['skeeterbytes',\n",
       " 'wrote',\n",
       " 'bassam',\n",
       " 'guy',\n",
       " 'wrote',\n",
       " \"n't\",\n",
       " 'anything',\n",
       " 'longer',\n",
       " '60mm',\n",
       " 'except',\n",
       " '$',\n",
       " '99',\n",
       " 'plastic',\n",
       " 'fantastic',\n",
       " \"''\",\n",
       " '40-150.',\n",
       " \"'ve\",\n",
       " 'never',\n",
       " 'really',\n",
       " 'tried',\n",
       " 'avian',\n",
       " 'photography',\n",
       " 'would',\n",
       " 'like',\n",
       " 'get',\n",
       " 'decent',\n",
       " 'shot',\n",
       " 'unlike',\n",
       " 'opportunity',\n",
       " 'arises',\n",
       " 'used',\n",
       " 'c-af+tr',\n",
       " 'retrospect',\n",
       " 'since',\n",
       " 'turkey',\n",
       " 'lawyer',\n",
       " 'refused',\n",
       " 'fly',\n",
       " 'despite',\n",
       " 'plea',\n",
       " 'would',\n",
       " 's-af',\n",
       " 'done',\n",
       " 'better',\n",
       " 'mf',\n",
       " 'peaking',\n",
       " 'magnify',\n",
       " 'exposed',\n",
       " 'lawyer',\n",
       " 'dressed',\n",
       " 'dark',\n",
       " 'suit',\n",
       " 'bright',\n",
       " 'cloudy',\n",
       " 'sky',\n",
       " 'meteorologist',\n",
       " 'say',\n",
       " 'smoke',\n",
       " 'remnant',\n",
       " 'ca',\n",
       " 'northern',\n",
       " 'va',\n",
       " 'wa',\n",
       " '+2',\n",
       " '+2.5',\n",
       " 'ev',\n",
       " 'much',\n",
       " 'encouraged',\n",
       " 'chroma',\n",
       " \"'m\",\n",
       " 'great',\n",
       " 'judging',\n",
       " 'distance',\n",
       " 'would',\n",
       " 'say',\n",
       " 'wa',\n",
       " '35+',\n",
       " 'meter',\n",
       " 'away',\n",
       " 'pretty',\n",
       " 'intense',\n",
       " 'purple',\n",
       " 'fringing',\n",
       " 'lightroom',\n",
       " 'ha',\n",
       " 'effective',\n",
       " 'one-click',\n",
       " 'remover',\n",
       " \"'m\",\n",
       " 'sure',\n",
       " 'software',\n",
       " 'ha',\n",
       " 'hard',\n",
       " 'control',\n",
       " 'know',\n",
       " 'panasonic',\n",
       " '7-14',\n",
       " '4',\n",
       " 'never',\n",
       " 'use',\n",
       " 'c-af+tr',\n",
       " 'preferring',\n",
       " 'standard',\n",
       " 'c-af',\n",
       " 's-af',\n",
       " 'instance',\n",
       " 's-af',\n",
       " 'fine',\n",
       " 'attorney',\n",
       " \"n't\",\n",
       " 'going',\n",
       " 'anywhere',\n",
       " 'pay',\n",
       " 'bill',\n",
       " 'get',\n",
       " 's-af+magnify',\n",
       " 'allow',\n",
       " 'tweak',\n",
       " 'focus',\n",
       " 'plus',\n",
       " 'want',\n",
       " 'stop',\n",
       " 'lens',\n",
       " 'bit',\n",
       " 'sharpen',\n",
       " 'except',\n",
       " '7-14',\n",
       " 'nothing',\n",
       " 'else',\n",
       " 'f2.8',\n",
       " '5.6',\n",
       " 'seems',\n",
       " 'stopped',\n",
       " \"'ll\",\n",
       " 'watch',\n",
       " 'tried',\n",
       " 'magnify',\n",
       " 'wa',\n",
       " 'shaky',\n",
       " 'long',\n",
       " 'fl',\n",
       " \"'ll\",\n",
       " 'try',\n",
       " 'lower',\n",
       " 'scale',\n",
       " 'factor',\n",
       " 'also',\n",
       " 'raise',\n",
       " 'shutter',\n",
       " 'speed',\n",
       " 'freeze',\n",
       " 'flapping',\n",
       " \"'re\",\n",
       " 'iso200',\n",
       " \"'s\",\n",
       " 'good',\n",
       " 'lens',\n",
       " 'within',\n",
       " 'realm',\n",
       " '40-150',\n",
       " 'pro',\n",
       " 'league',\n",
       " 'better',\n",
       " '10x',\n",
       " 'price',\n",
       " 'birding',\n",
       " 'test',\n",
       " 'petapixel',\n",
       " 'fstoppers',\n",
       " 'rated',\n",
       " 'e-m5',\n",
       " 'iii',\n",
       " 'pretty',\n",
       " 'well',\n",
       " 'c-af+tr',\n",
       " \"n't\",\n",
       " 'use',\n",
       " 'often',\n",
       " 'location',\n",
       " '5',\n",
       " 'minute',\n",
       " 'walk',\n",
       " 'home',\n",
       " 'perhaps',\n",
       " \"'ll\",\n",
       " 'try',\n",
       " 'keep',\n",
       " 'experimenting',\n",
       " 'rick',\n",
       " 'thanks',\n",
       " 'helpful',\n",
       " 'reply']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "TextCleanPonctuationWord = [word for word in TextCleanPonctuation if word not in stoplist]\n",
    "TextCleanPonctuationWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(singleText):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_punctuation = [':', '(', ')', '/', '|', ',', ']', ';',\n",
    "                    '.', '*', '#', '\"', '&', '~', '``',\n",
    "                    '-', '_', '\\\\', '@','?','!','\\'', '[']\n",
    "    stoplist = stopwords.words('english')\n",
    "\n",
    "    LowerSingleText = singleText.lower()\n",
    "    tokenizeSingletext = word_tokenize(LowerSingleText)\n",
    "    lemmatizeSingleText = list(map(lemmatizer.lemmatize, tokenizeSingletext))\n",
    "    \n",
    "    TextCleanPonctuation = [word for word in lemmatizeSingleText if word not in stop_punctuation and word not in stoplist]\n",
    "\n",
    "    return TextCleanPonctuation\n",
    "\n",
    "def clean_text(text):\n",
    "    text_preprocessed = preprocessing(text)\n",
    "    \n",
    "    clean_mail = [word for word in text_preprocessed if word not in stop_punctuation and not word in stoplist]\n",
    "    return ' '.join(clean_mail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"skeeterbytes wrote bassam guy wrote n't anything longer 60mm except $ 99 plastic fantastic '' 40-150. 've never really tried avian photography would like get decent shot unlike opportunity arises used c-af+tr retrospect since turkey lawyer refused fly despite plea would s-af done better mf peaking magnify exposed lawyer dressed dark suit bright cloudy sky meteorologist say smoke remnant ca northern va wa +2 +2.5 ev much encouraged chroma 'm great judging distance would say wa 35+ meter away pretty intense purple fringing lightroom ha effective one-click remover 'm sure software ha hard control know panasonic 7-14 4 never use c-af+tr preferring standard c-af s-af instance s-af fine attorney n't going anywhere pay bill get s-af+magnify allow tweak focus plus want stop lens bit sharpen except 7-14 nothing else f2.8 5.6 seems stopped 'll watch tried magnify wa shaky long fl 'll try lower scale factor also raise shutter speed freeze flapping 're iso200 's good lens within realm 40-150 pro league better 10x price birding test petapixel fstoppers rated e-m5 iii pretty well c-af+tr n't use often location 5 minute walk home perhaps 'll try keep experimenting rick thanks helpful reply\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = clean_text(df_train[\"text\"].iloc[0])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On applique cela pour toute la base de donn√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        skeeterbytes wrote bassam guy wrote n't anythi...\n",
       "1        refueling long week üçµ ‚Äô meant drink prefer üçµ m...\n",
       "2        case one ha told today strong beautiful love üåπ...\n",
       "3        forget get sunshine safely ‚òÄÔ∏è head link bio se...\n",
       "4        abstract background human dramatically changed...\n",
       "                               ...                        \n",
       "10567    ‚òë day 273 outifit thor gold foil back bling et...\n",
       "10568    order study various trend pattern prevailing c...\n",
       "10569    brand ha yet registered influenster work brand...\n",
       "10570    regime get rid acne scar get way smoother heal...\n",
       "10571    's true used work caddie trump national nj don...\n",
       "Name: clean_content, Length: 10572, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"clean_content\"] = df_train.text.apply(clean_text)\n",
    "df_train[\"clean_content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        skeeterbytes wrote bassam guy wrote n't anythi...\n",
       "1        refueling long week üçµ ‚Äô meant drink prefer üçµ m...\n",
       "2        case one ha told today strong beautiful love üåπ...\n",
       "3        forget get sunshine safely ‚òÄÔ∏è head link bio se...\n",
       "4        abstract background human dramatically changed...\n",
       "                               ...                        \n",
       "10567    ‚òë day 273 outifit thor gold foil back bling et...\n",
       "10568    order study various trend pattern prevailing c...\n",
       "10569    brand ha yet registered influenster work brand...\n",
       "10570    regime get rid acne scar get way smoother heal...\n",
       "10571    's true used work caddie trump national nj don...\n",
       "Name: clean_content, Length: 10572, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_train['clean_content']\n",
    "Y = df_train.drop(['clean_content', 'index'], axis = 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8457x71330 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 704785 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(CountVectorizer(), TfidfTransformer())\n",
    "pipe.fit(x_train)\n",
    "feat_train = pipe.transform(x_train)\n",
    "feat_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8457x71330 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 704785 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "clf.fit(feat_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_test = pipe.transform(x_test)\n",
    "clf.score(feat_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualisation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "score = clf.predict_proba(feat_test)\n",
    "score\n",
    "#fpr, tpr, th = roc_curve(y_list_test, score[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "aucf = auc(fpr, tpr)\n",
    "ax.plot(fpr, tpr, label='auc=%1.5f' % aucf)\n",
    "ax.set_title('Courbe ROC - classifieur de sentiments')\n",
    "ax.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}