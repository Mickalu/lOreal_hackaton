{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>skincare</th>\n",
       "      <th>hair</th>\n",
       "      <th>make-up</th>\n",
       "      <th>other</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Skeeterbytes wrote: Bassam Guy wrote: I don't ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Refueling before the long week üçµ - You‚Äôre mean...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Just in case no one has told you today YOU ARE...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Do not forget to get some sunshine safely if y...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Abstract Background Humans have dramatically c...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10567</th>\n",
       "      <td>13209</td>\n",
       "      <td>‚òë Day: 273 Outifit: Thor (Gold Foil) Back Blin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10568</th>\n",
       "      <td>13210</td>\n",
       "      <td>In order to study the various trends and patte...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10569</th>\n",
       "      <td>13212</td>\n",
       "      <td>This brand has not yet registered with Influen...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10570</th>\n",
       "      <td>13213</td>\n",
       "      <td>here is my regime get rid of any acne scars an...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10571</th>\n",
       "      <td>13214</td>\n",
       "      <td>It's true, I used to work as a caddie at Trump...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10572 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               text  \\\n",
       "0               0  Skeeterbytes wrote: Bassam Guy wrote: I don't ...   \n",
       "1               1  Refueling before the long week üçµ - You‚Äôre mean...   \n",
       "2               2  Just in case no one has told you today YOU ARE...   \n",
       "3               3  Do not forget to get some sunshine safely if y...   \n",
       "4               4  Abstract Background Humans have dramatically c...   \n",
       "...           ...                                                ...   \n",
       "10567       13209  ‚òë Day: 273 Outifit: Thor (Gold Foil) Back Blin...   \n",
       "10568       13210  In order to study the various trends and patte...   \n",
       "10569       13212  This brand has not yet registered with Influen...   \n",
       "10570       13213  here is my regime get rid of any acne scars an...   \n",
       "10571       13214  It's true, I used to work as a caddie at Trump...   \n",
       "\n",
       "       skincare  hair  make-up  other  index  \n",
       "0           0.0   0.0      0.0    1.0      0  \n",
       "1           0.0   0.0      0.0    1.0      1  \n",
       "2           0.0   0.0      1.0    0.0      2  \n",
       "3           0.0   0.0      0.0    1.0      3  \n",
       "4           0.0   0.0      0.0    1.0      4  \n",
       "...         ...   ...      ...    ...    ...  \n",
       "10567       0.0   0.0      0.0    1.0  13209  \n",
       "10568       0.0   0.0      0.0    1.0  13210  \n",
       "10569       0.0   0.0      0.0    1.0  13212  \n",
       "10570       1.0   0.0      0.0    0.0  13213  \n",
       "10571       0.0   0.0      0.0    1.0  13214  \n",
       "\n",
       "[10572 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"data/hackathon_loreal_train_set.csv\")\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# watch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'text', 'skincare', 'hair', 'make-up', 'other', 'index'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>skincare</th>\n",
       "      <th>hair</th>\n",
       "      <th>make-up</th>\n",
       "      <th>other</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, text, skincare, hair, make-up, other, index]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_train[df_train['Unnamed: 0'] != df_train['index']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# delete one columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>skincare</th>\n",
       "      <th>hair</th>\n",
       "      <th>make-up</th>\n",
       "      <th>other</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skeeterbytes wrote: Bassam Guy wrote: I don't ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Refueling before the long week üçµ - You‚Äôre mean...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just in case no one has told you today YOU ARE...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Do not forget to get some sunshine safely if y...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abstract Background Humans have dramatically c...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10567</th>\n",
       "      <td>‚òë Day: 273 Outifit: Thor (Gold Foil) Back Blin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10568</th>\n",
       "      <td>In order to study the various trends and patte...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10569</th>\n",
       "      <td>This brand has not yet registered with Influen...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10570</th>\n",
       "      <td>here is my regime get rid of any acne scars an...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10571</th>\n",
       "      <td>It's true, I used to work as a caddie at Trump...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10572 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  skincare  hair  \\\n",
       "0      Skeeterbytes wrote: Bassam Guy wrote: I don't ...       0.0   0.0   \n",
       "1      Refueling before the long week üçµ - You‚Äôre mean...       0.0   0.0   \n",
       "2      Just in case no one has told you today YOU ARE...       0.0   0.0   \n",
       "3      Do not forget to get some sunshine safely if y...       0.0   0.0   \n",
       "4      Abstract Background Humans have dramatically c...       0.0   0.0   \n",
       "...                                                  ...       ...   ...   \n",
       "10567  ‚òë Day: 273 Outifit: Thor (Gold Foil) Back Blin...       0.0   0.0   \n",
       "10568  In order to study the various trends and patte...       0.0   0.0   \n",
       "10569  This brand has not yet registered with Influen...       0.0   0.0   \n",
       "10570  here is my regime get rid of any acne scars an...       1.0   0.0   \n",
       "10571  It's true, I used to work as a caddie at Trump...       0.0   0.0   \n",
       "\n",
       "       make-up  other  index  \n",
       "0          0.0    1.0      0  \n",
       "1          0.0    1.0      1  \n",
       "2          1.0    0.0      2  \n",
       "3          0.0    1.0      3  \n",
       "4          0.0    1.0      4  \n",
       "...        ...    ...    ...  \n",
       "10567      0.0    1.0  13209  \n",
       "10568      0.0    1.0  13210  \n",
       "10569      0.0    1.0  13212  \n",
       "10570      0.0    0.0  13213  \n",
       "10571      0.0    1.0  13214  \n",
       "\n",
       "[10572 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.drop(['Unnamed: 0'], axis = 1)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing one text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Thomas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from nltk import word_tokenize, WordNetLemmatizer, PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk import ngrams\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Refueling before the long week üçµ - You‚Äôre meant to Drink it, but I prefer it in my üçµ From to , matcha is a natural powder of freshly ground leaves! ‚úÖthanks to its numerous antioxidant benefits! ‚úÖIncluding it in your beauty routine requires very little time and effort to see benefits ‚úÖ The best part of it is that it‚Äôs already in your cupboard and doubles as a delicious drink? It really doesn‚Äôt get better than this! üõí Matcha tea purchased from my favorite online store faithfultonature'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Onetext = df_train['text'].iloc[1]\n",
    "Onetext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'refueling before the long week üçµ - you‚Äôre meant to drink it, but i prefer it in my üçµ from to , matcha is a natural powder of freshly ground leaves! ‚úÖthanks to its numerous antioxidant benefits! ‚úÖincluding it in your beauty routine requires very little time and effort to see benefits ‚úÖ the best part of it is that it‚Äôs already in your cupboard and doubles as a delicious drink? it really doesn‚Äôt get better than this! üõí matcha tea purchased from my favorite online store faithfultonature'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OnetextLower = Onetext.lower()\n",
    "OnetextLower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['refueling',\n",
       " 'before',\n",
       " 'the',\n",
       " 'long',\n",
       " 'week',\n",
       " 'üçµ',\n",
       " '-',\n",
       " 'you',\n",
       " '‚Äô',\n",
       " 're',\n",
       " 'meant',\n",
       " 'to',\n",
       " 'drink',\n",
       " 'it',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'prefer',\n",
       " 'it',\n",
       " 'in',\n",
       " 'my',\n",
       " 'üçµ',\n",
       " 'from',\n",
       " 'to',\n",
       " ',',\n",
       " 'matcha',\n",
       " 'is',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'powder',\n",
       " 'of',\n",
       " 'freshly',\n",
       " 'ground',\n",
       " 'leaves',\n",
       " '!',\n",
       " '‚úÖthanks',\n",
       " 'to',\n",
       " 'its',\n",
       " 'numerous',\n",
       " 'antioxidant',\n",
       " 'benefits',\n",
       " '!',\n",
       " '‚úÖincluding',\n",
       " 'it',\n",
       " 'in',\n",
       " 'your',\n",
       " 'beauty',\n",
       " 'routine',\n",
       " 'requires',\n",
       " 'very',\n",
       " 'little',\n",
       " 'time',\n",
       " 'and',\n",
       " 'effort',\n",
       " 'to',\n",
       " 'see',\n",
       " 'benefits',\n",
       " '‚úÖ',\n",
       " 'the',\n",
       " 'best',\n",
       " 'part',\n",
       " 'of',\n",
       " 'it',\n",
       " 'is',\n",
       " 'that',\n",
       " 'it',\n",
       " '‚Äô',\n",
       " 's',\n",
       " 'already',\n",
       " 'in',\n",
       " 'your',\n",
       " 'cupboard',\n",
       " 'and',\n",
       " 'doubles',\n",
       " 'as',\n",
       " 'a',\n",
       " 'delicious',\n",
       " 'drink',\n",
       " '?',\n",
       " 'it',\n",
       " 'really',\n",
       " 'doesn',\n",
       " '‚Äô',\n",
       " 't',\n",
       " 'get',\n",
       " 'better',\n",
       " 'than',\n",
       " 'this',\n",
       " '!',\n",
       " 'üõí',\n",
       " 'matcha',\n",
       " 'tea',\n",
       " 'purchased',\n",
       " 'from',\n",
       " 'my',\n",
       " 'favorite',\n",
       " 'online',\n",
       " 'store',\n",
       " 'faithfultonature']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizeSingletext = word_tokenize(OnetextLower)\n",
    "tokenizeSingletext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['refueling',\n",
       " 'before',\n",
       " 'the',\n",
       " 'long',\n",
       " 'week',\n",
       " 'üçµ',\n",
       " '-',\n",
       " 'you',\n",
       " '‚Äô',\n",
       " 're',\n",
       " 'meant',\n",
       " 'to',\n",
       " 'drink',\n",
       " 'it',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'prefer',\n",
       " 'it',\n",
       " 'in',\n",
       " 'my',\n",
       " 'üçµ',\n",
       " 'from',\n",
       " 'to',\n",
       " ',',\n",
       " 'matcha',\n",
       " 'is',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'powder',\n",
       " 'of',\n",
       " 'freshly',\n",
       " 'ground',\n",
       " 'leaf',\n",
       " '!',\n",
       " '‚úÖthanks',\n",
       " 'to',\n",
       " 'it',\n",
       " 'numerous',\n",
       " 'antioxidant',\n",
       " 'benefit',\n",
       " '!',\n",
       " '‚úÖincluding',\n",
       " 'it',\n",
       " 'in',\n",
       " 'your',\n",
       " 'beauty',\n",
       " 'routine',\n",
       " 'requires',\n",
       " 'very',\n",
       " 'little',\n",
       " 'time',\n",
       " 'and',\n",
       " 'effort',\n",
       " 'to',\n",
       " 'see',\n",
       " 'benefit',\n",
       " '‚úÖ',\n",
       " 'the',\n",
       " 'best',\n",
       " 'part',\n",
       " 'of',\n",
       " 'it',\n",
       " 'is',\n",
       " 'that',\n",
       " 'it',\n",
       " '‚Äô',\n",
       " 's',\n",
       " 'already',\n",
       " 'in',\n",
       " 'your',\n",
       " 'cupboard',\n",
       " 'and',\n",
       " 'double',\n",
       " 'a',\n",
       " 'a',\n",
       " 'delicious',\n",
       " 'drink',\n",
       " '?',\n",
       " 'it',\n",
       " 'really',\n",
       " 'doesn',\n",
       " '‚Äô',\n",
       " 't',\n",
       " 'get',\n",
       " 'better',\n",
       " 'than',\n",
       " 'this',\n",
       " '!',\n",
       " 'üõí',\n",
       " 'matcha',\n",
       " 'tea',\n",
       " 'purchased',\n",
       " 'from',\n",
       " 'my',\n",
       " 'favorite',\n",
       " 'online',\n",
       " 'store',\n",
       " 'faithfultonature']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizeSingleText = list(map(lemmatizer.lemmatize, tokenizeSingletext))\n",
    "lemmatizeSingleText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['refueling',\n",
       " 'before',\n",
       " 'the',\n",
       " 'long',\n",
       " 'week',\n",
       " 'you',\n",
       " '‚Äô',\n",
       " 're',\n",
       " 'meant',\n",
       " 'to',\n",
       " 'drink',\n",
       " 'it',\n",
       " 'but',\n",
       " 'i',\n",
       " 'prefer',\n",
       " 'it',\n",
       " 'in',\n",
       " 'my',\n",
       " 'from',\n",
       " 'to',\n",
       " 'matcha',\n",
       " 'is',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'powder',\n",
       " 'of',\n",
       " 'freshly',\n",
       " 'ground',\n",
       " 'leaf',\n",
       " '‚úÖthanks',\n",
       " 'to',\n",
       " 'it',\n",
       " 'numerous',\n",
       " 'antioxidant',\n",
       " 'benefit',\n",
       " '‚úÖincluding',\n",
       " 'it',\n",
       " 'in',\n",
       " 'your',\n",
       " 'beauty',\n",
       " 'routine',\n",
       " 'requires',\n",
       " 'very',\n",
       " 'little',\n",
       " 'time',\n",
       " 'and',\n",
       " 'effort',\n",
       " 'to',\n",
       " 'see',\n",
       " 'benefit',\n",
       " '‚úÖ',\n",
       " 'the',\n",
       " 'best',\n",
       " 'part',\n",
       " 'of',\n",
       " 'it',\n",
       " 'is',\n",
       " 'that',\n",
       " 'it',\n",
       " '‚Äô',\n",
       " 's',\n",
       " 'already',\n",
       " 'in',\n",
       " 'your',\n",
       " 'cupboard',\n",
       " 'and',\n",
       " 'double',\n",
       " 'a',\n",
       " 'a',\n",
       " 'delicious',\n",
       " 'drink',\n",
       " 'it',\n",
       " 'really',\n",
       " 'doesn',\n",
       " '‚Äô',\n",
       " 't',\n",
       " 'get',\n",
       " 'better',\n",
       " 'than',\n",
       " 'this',\n",
       " 'üõí',\n",
       " 'matcha',\n",
       " 'tea',\n",
       " 'purchased',\n",
       " 'from',\n",
       " 'my',\n",
       " 'favorite',\n",
       " 'online',\n",
       " 'store',\n",
       " 'faithfultonature']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_punctuation = [':', '(', ')', '/', '|', ',', ']', ';',\n",
    "                    '.', '*', '#', '\"', '&', '~', '``',\n",
    "                    '-', '_', '\\\\', '@','?','!','\\'', '[', 'üçµ']\n",
    "TextCleanPonctuation = [word for word in lemmatizeSingleText if word not in stop_punctuation]\n",
    "TextCleanPonctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['refueling',\n",
       " 'long',\n",
       " 'week',\n",
       " '‚Äô',\n",
       " 'meant',\n",
       " 'drink',\n",
       " 'prefer',\n",
       " 'matcha',\n",
       " 'natural',\n",
       " 'powder',\n",
       " 'freshly',\n",
       " 'ground',\n",
       " 'leaf',\n",
       " '‚úÖthanks',\n",
       " 'numerous',\n",
       " 'antioxidant',\n",
       " 'benefit',\n",
       " '‚úÖincluding',\n",
       " 'beauty',\n",
       " 'routine',\n",
       " 'requires',\n",
       " 'little',\n",
       " 'time',\n",
       " 'effort',\n",
       " 'see',\n",
       " 'benefit',\n",
       " '‚úÖ',\n",
       " 'best',\n",
       " 'part',\n",
       " '‚Äô',\n",
       " 'already',\n",
       " 'cupboard',\n",
       " 'double',\n",
       " 'delicious',\n",
       " 'drink',\n",
       " 'really',\n",
       " '‚Äô',\n",
       " 'get',\n",
       " 'better',\n",
       " 'üõí',\n",
       " 'matcha',\n",
       " 'tea',\n",
       " 'purchased',\n",
       " 'favorite',\n",
       " 'online',\n",
       " 'store',\n",
       " 'faithfultonature']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "TextCleanPonctuationWord = [word for word in TextCleanPonctuation if word not in stoplist]\n",
    "TextCleanPonctuationWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(singleText):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_punctuation = [':', '(', ')', '/', '|', ',', ']', ';',\n",
    "                    '.', '*', '#', '\"', '&', '~', '``',\n",
    "                    '-', '_', '\\\\', '@','?','!','\\'', '[']\n",
    "    stoplist = stopwords.words('english')\n",
    "\n",
    "    LowerSingleText = singleText.lower()\n",
    "    tokenizeSingletext = word_tokenize(LowerSingleText)\n",
    "    lemmatizeSingleText = list(map(lemmatizer.lemmatize, tokenizeSingletext))\n",
    "    \n",
    "    TextCleanPonctuation = [word for word in lemmatizeSingleText if word not in stop_punctuation and word not in stoplist]\n",
    "\n",
    "    return TextCleanPonctuation\n",
    "\n",
    "def clean_text(text):\n",
    "    text_preprocessed = preprocessing(text)\n",
    "    \n",
    "    clean_mail = [word for word in text_preprocessed if word not in stop_punctuation and not word in stoplist]\n",
    "    return ' '.join(clean_mail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"skeeterbytes wrote bassam guy wrote n't anything longer 60mm except $ 99 plastic fantastic '' 40-150. 've never really tried avian photography would like get decent shot unlike opportunity arises used c-af+tr retrospect since turkey lawyer refused fly despite plea would s-af done better mf peaking magnify exposed lawyer dressed dark suit bright cloudy sky meteorologist say smoke remnant ca northern va wa +2 +2.5 ev much encouraged chroma 'm great judging distance would say wa 35+ meter away pretty intense purple fringing lightroom ha effective one-click remover 'm sure software ha hard control know panasonic 7-14 4 never use c-af+tr preferring standard c-af s-af instance s-af fine attorney n't going anywhere pay bill get s-af+magnify allow tweak focus plus want stop lens bit sharpen except 7-14 nothing else f2.8 5.6 seems stopped 'll watch tried magnify wa shaky long fl 'll try lower scale factor also raise shutter speed freeze flapping 're iso200 's good lens within realm 40-150 pro league better 10x price birding test petapixel fstoppers rated e-m5 iii pretty well c-af+tr n't use often location 5 minute walk home perhaps 'll try keep experimenting rick thanks helpful reply\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = clean_text(df_train[\"text\"].iloc[0])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On applique cela pour toute la base de donn√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        skeeterbytes wrote bassam guy wrote n't anythi...\n",
       "1        refueling long week ‚Äô meant drink prefer match...\n",
       "2        case one ha told today strong beautiful love üåπ...\n",
       "3        forget get sunshine safely ‚òÄÔ∏è head link bio se...\n",
       "4        abstract background human dramatically changed...\n",
       "                               ...                        \n",
       "10567    ‚òë day 273 outifit thor gold foil back bling et...\n",
       "10568    order study various trend pattern prevailing c...\n",
       "10569    brand ha yet registered influenster work brand...\n",
       "10570    regime get rid acne scar get way smoother heal...\n",
       "10571    's true used work caddie trump national nj don...\n",
       "Name: clean_content, Length: 10572, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"clean_content\"] = df_train.text.apply(clean_text)\n",
    "df_train[\"clean_content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        skeeterbytes wrote bassam guy wrote n't anythi...\n",
       "1        refueling long week ‚Äô meant drink prefer match...\n",
       "2        case one ha told today strong beautiful love üåπ...\n",
       "3        forget get sunshine safely ‚òÄÔ∏è head link bio se...\n",
       "4        abstract background human dramatically changed...\n",
       "                               ...                        \n",
       "10567    ‚òë day 273 outifit thor gold foil back bling et...\n",
       "10568    order study various trend pattern prevailing c...\n",
       "10569    brand ha yet registered influenster work brand...\n",
       "10570    regime get rid acne scar get way smoother heal...\n",
       "10571    's true used work caddie trump national nj don...\n",
       "Name: clean_content, Length: 10572, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_train['clean_content']\n",
    "Y = df_train.drop(['clean_content', 'index'], axis = 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8457x71330 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 704785 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(CountVectorizer(), TfidfTransformer())\n",
    "pipe.fit(x_train)\n",
    "feat_train = pipe.transform(x_train)\n",
    "feat_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8457x71330 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 704785 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-29ec9c4f0ecf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_class_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_y_class_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mDOUBLE\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_validate_y_class_weight\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_y_class_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \"\"\"\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[0my_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[0;32m    171\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n",
      "\u001b[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be class 'SparseSeries' or 'SparseArray'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mis_multilabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m'multilabel-indicator'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mis_multilabel\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    150\u001b[0m                  _is_integral_float(np.unique(y.data))))\n\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         return len(labels) < 3 and (y.dtype.kind in 'biu' or  # bool, int, uint\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[0mar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m         \u001b[0mar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m         \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "clf.fit(feat_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_test = pipe.transform(x_test)\n",
    "clf.score(feat_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualisation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "score = clf.predict_proba(feat_test)\n",
    "score\n",
    "#fpr, tpr, th = roc_curve(y_list_test, score[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "aucf = auc(fpr, tpr)\n",
    "ax.plot(fpr, tpr, label='auc=%1.5f' % aucf)\n",
    "ax.set_title('Courbe ROC - classifieur de sentiments')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Refueling before the long week üçµ - You‚Äôre meant to Drink it, but I prefer it in my üçµ From to , matcha is a natural powder of freshly ground leaves! ‚úÖthanks to its numerous antioxidant benefits! ‚úÖIncluding it in your beauty routine requires very little time and effort to see benefits ‚úÖ The best part of it is that it‚Äôs already in your cupboard and doubles as a delicious drink? It really doesn‚Äôt get better than this! üõí Matcha tea purchased from my favorite online store faithfultonature'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Refueling before the long week  - You‚Äôre meant to Drink it, but I prefer it in my  From to , matcha is a natural powder of freshly ground leaves! thanks to its numerous antioxidant benefits! Including it in your beauty routine requires very little time and effort to see benefits  The best part of it is that it‚Äôs already in your cupboard and doubles as a delicious drink? It really doesn‚Äôt get better than this!  Matcha tea purchased from my favorite online store faithfultonature'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji(df_train.text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "# Function for removing emoticons\n",
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Refueling before the long week üçµ - You‚Äôre meant to Drink it, but I prefer it in my üçµ From to , matcha is a natural powder of freshly ground leaves! ‚úÖthanks to its numerous antioxidant benefits! ‚úÖIncluding it in your beauty routine requires very little time and effort to see benefits ‚úÖ The best part of it is that it‚Äôs already in your cupboard and doubles as a delicious drink? It really doesn‚Äôt get better than this! üõí Matcha tea purchased from my favorite online store faithfultonature'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoticons(df_train.text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle  \n",
    "\n",
    "with open('Emoticon_Dict.p', 'rb') as fp:\n",
    "    Emoticon_Dict = pickle.load(fp)\n",
    "\n",
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in Emoticon_Dict) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Emoji_Dict.p', 'rb') as fp:\n",
    "    Emoji_Dict = pickle.load(fp)\n",
    "Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n",
    "\n",
    "def convert_emojis_to_word(text):\n",
    "    for emot in Emoji_Dict:\n",
    "        text = re.sub(r'('+emot+')', \"_\".join(Emoji_Dict[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Refueling before the long week teacup_without_handle - You‚Äôre meant to Drink it, but I prefer it in my teacup_without_handle From to , matcha is a natural powder of freshly ground leaves! white_heavy_check_markthanks to its numerous antioxidant benefits! white_heavy_check_markIncluding it in your beauty routine requires very little time and effort to see benefits white_heavy_check_mark The best part of it is that it‚Äôs already in your cupboard and doubles as a delicious drink? It really doesn‚Äôt get better than this! shopping_cart Matcha tea purchased from my favorite online store faithfultonature'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_emojis_to_word(df_train.text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lipstick\n",
    "man_getting_haircut\n",
    "man_getting_haircut_dark_skin_tone\n",
    "man_getting_haircut_light_skin_tone\n",
    "man_getting_haircut_medium-dark_skin_tone\n",
    "man_getting_haircut_medium-light_skin_tone\n",
    "man_getting_haircut_medium_skin_tone\n",
    "man_getting_massage\n",
    "man_getting_massage_dark_skin_tone\n",
    "man_getting_massage_light_skin_tone\n",
    "man_getting_massage_medium-dark_skin_tone\n",
    "man_getting_massage_medium-light_skin_tone\n",
    "man_getting_massage_medium_skin_tone\n",
    "nail_polish\n",
    "nail_polish_dark_skin_tone\n",
    "nail_polish_light_skin_tone\n",
    "nail_polish_medium-dark_skin_tone\n",
    "nail_polish_medium-light_skin_tone\n",
    "nail_polish_medium_skin_tone\n",
    "woman_getting_haircut\n",
    "woman_getting_haircut_dark_skin_tone\n",
    "woman_getting_haircut_light_skin_tone\n",
    "woman_getting_haircut_medium-dark_skin_tone\n",
    "woman_getting_haircut_medium-light_skin_tone\n",
    "woman_getting_haircut_medium_skin_tone\n",
    "woman_getting_massage\n",
    "woman_getting_massage_dark_skin_tone\n",
    "woman_getting_massage_light_skin_tone\n",
    "woman_getting_massage_medium-dark_skin_tone\n",
    "woman_getting_massage_medium-light_skin_tone\n",
    "woman_getting_massage_medium_skin_tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de posts avec des emojis : 3282 || Pourcentage (%) : 31.04426787741203\n"
     ]
    }
   ],
   "source": [
    "def test_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    if re.search(emoji_pattern, string):\n",
    "        return 1\n",
    "    else: return 0\n",
    "    \n",
    "compteur = 0\n",
    "for i in df_train.index:\n",
    "    compteur += test_emoji(df_train.text.iloc[i])\n",
    "\n",
    "print(\"Nombre de posts avec des emojis :\", compteur, \"|| Pourcentage (%) :\", (compteur/df_train.shape[0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de posts de cat√©gorie skincare : 1303 || Pourcentage (%) : 12.325009458948164\n",
      "Nombre de posts de cat√©gorie hair : 1272 || Pourcentage (%) : 12.031782065834278\n",
      "Nombre de posts de cat√©gorie make-up : 1907 || Pourcentage (%) : 18.038214150586455\n",
      "Nombre de posts de cat√©gorie other : 6667 || Pourcentage (%) : 63.06280741581536\n"
     ]
    }
   ],
   "source": [
    "list_categories = [\"skincare\", \"hair\", \"make-up\", \"other\"]\n",
    "\n",
    "for category in list_categories:\n",
    "    print(\"Nombre de posts de cat√©gorie\", category, \":\", len(df_train[df_train[category] == 1]),\n",
    "          \"|| Pourcentage (%) :\", (len(df_train[df_train[category] == 1]) / df_train.shape[0]) * 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.text.apply(convert_emojis_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de posts avec des urls : 21 || Pourcentage (%) : 0.19863791146424517\n"
     ]
    }
   ],
   "source": [
    "def test_url(string):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    if re.search(url_pattern, string):\n",
    "        return 1\n",
    "    else: return 0\n",
    "    \n",
    "compteur_2 = 0\n",
    "for i in df_train.index:\n",
    "    compteur_2 += test_url(df_train.text.iloc[i])\n",
    "\n",
    "print(\"Nombre de posts avec des urls :\", compteur_2, \"|| Pourcentage (%) :\", (compteur_2/df_train.shape[0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de posts avec des 's : 420 || Pourcentage (%) : 3.9727582292849033\n"
     ]
    }
   ],
   "source": [
    "def test_s(string):\n",
    "    url_pattern = re.compile(r\"--\")\n",
    "    if re.search(url_pattern, string):\n",
    "        return 1\n",
    "    else: return 0\n",
    "    \n",
    "compteur_3 = 0\n",
    "for i in df_train.index:\n",
    "    compteur_3 += test_s(df_train.text.iloc[i])\n",
    "\n",
    "print(\"Nombre de posts avec des 's :\", compteur_3, \"|| Pourcentage (%) :\", (compteur_3/df_train.shape[0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Skeeterbytes wrote: Bassam Guy wrote: I don\\'t have anything longer than 60mm except my $99 \"plastic [not so] fantastic\" 40-150. I\\'ve never really tried avian photography before but would like to get some decent shots (unlike these) when the opportunity arises. I used C-AF+TR. In retrospect, since the Turkey Lawyer refused to fly despite my pleas. Would S-AF have done better? MF, Peaking or Magnify or both? I exposed for the lawyer, dressed in dark suits against a bright cloudy sky (some meteorologists say it is smoke remnants from CA in Northern VA). Was +2 and +2.5 EV too much and encouraged the chroma? I\\'m not great at judging distance but I would say I was about 35+ meters away. Pretty intense purple fringing. Lightroom has an effective one-click remover and I\\'m sure other software has, too. That should not be hard to control. I know how to do that. I own the Panasonic 7-14 4 Never use C-AF+tr myself, preferring standard C-AF or S-AF. In this instance, S-AF is fine because your attorney isn\\'t going anywhere until you pay your bill [get it?] and S-AF+magnify will allow you to tweak focus. Plus, you want to stop down the lens a bit and it will sharpen; Except for the 7-14, I have nothing else above f2.8. 5.6 seems stopped down. I\\'ll watch it. I tried magnify but it was too shaky at the long FL. I\\'ll try a lower scale factor. also, raise shutter speed to freeze the flapping about (you\\'re below ISO200). It\\'s a good lens within its realm but the 40-150 Pro is leagues better. As it should be at 10X the price. Some birding test from petapixel or fstoppers or such rated the E-M5 III pretty well for C-AF+TR. I don\\'t use it often. That location is a 5 minute walk from my home. Perhaps I\\'ll try again. Keep experimenting! Rick Thanks for your helpful reply.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.text[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
