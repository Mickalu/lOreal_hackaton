{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from nltk import word_tokenize, WordNetLemmatizer, PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk import ngrams\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_col = ['skincare', 'hair', 'make-up', 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       Unnamed: 0                                               text  \\\n",
       "0               0  Skeeterbytes wrote: Bassam Guy wrote: I don't ...   \n",
       "1               1  Refueling before the long week üçµ - You‚Äôre mean...   \n",
       "2               2  Just in case no one has told you today YOU ARE...   \n",
       "3               3  Do not forget to get some sunshine safely if y...   \n",
       "4               4  Abstract Background Humans have dramatically c...   \n",
       "...           ...                                                ...   \n",
       "10567       13209  ‚òë Day: 273 Outifit: Thor (Gold Foil) Back Blin...   \n",
       "10568       13210  In order to study the various trends and patte...   \n",
       "10569       13212  This brand has not yet registered with Influen...   \n",
       "10570       13213  here is my regime get rid of any acne scars an...   \n",
       "10571       13214  It's true, I used to work as a caddie at Trump...   \n",
       "\n",
       "       skincare  hair  make-up  other  index  \n",
       "0           0.0   0.0      0.0    1.0      0  \n",
       "1           0.0   0.0      0.0    1.0      1  \n",
       "2           0.0   0.0      1.0    0.0      2  \n",
       "3           0.0   0.0      0.0    1.0      3  \n",
       "4           0.0   0.0      0.0    1.0      4  \n",
       "...         ...   ...      ...    ...    ...  \n",
       "10567       0.0   0.0      0.0    1.0  13209  \n",
       "10568       0.0   0.0      0.0    1.0  13210  \n",
       "10569       0.0   0.0      0.0    1.0  13212  \n",
       "10570       1.0   0.0      0.0    0.0  13213  \n",
       "10571       0.0   0.0      0.0    1.0  13214  \n",
       "\n",
       "[10572 rows x 7 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>text</th>\n      <th>skincare</th>\n      <th>hair</th>\n      <th>make-up</th>\n      <th>other</th>\n      <th>index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Skeeterbytes wrote: Bassam Guy wrote: I don't ...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Refueling before the long week üçµ - You‚Äôre mean...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Just in case no one has told you today YOU ARE...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Do not forget to get some sunshine safely if y...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Abstract Background Humans have dramatically c...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10567</th>\n      <td>13209</td>\n      <td>‚òë Day: 273 Outifit: Thor (Gold Foil) Back Blin...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>13209</td>\n    </tr>\n    <tr>\n      <th>10568</th>\n      <td>13210</td>\n      <td>In order to study the various trends and patte...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>13210</td>\n    </tr>\n    <tr>\n      <th>10569</th>\n      <td>13212</td>\n      <td>This brand has not yet registered with Influen...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>13212</td>\n    </tr>\n    <tr>\n      <th>10570</th>\n      <td>13213</td>\n      <td>here is my regime get rid of any acne scars an...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>13213</td>\n    </tr>\n    <tr>\n      <th>10571</th>\n      <td>13214</td>\n      <td>It's true, I used to work as a caddie at Trump...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>13214</td>\n    </tr>\n  </tbody>\n</table>\n<p>10572 rows √ó 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"data/hackathon_loreal_train_set.csv\")\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# watch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'text', 'skincare', 'hair', 'make-up', 'other', 'index'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# delete one columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    text  skincare  hair  \\\n",
       "0      Skeeterbytes wrote: Bassam Guy wrote: I don't ...       0.0   0.0   \n",
       "1      Refueling before the long week üçµ - You‚Äôre mean...       0.0   0.0   \n",
       "2      Just in case no one has told you today YOU ARE...       0.0   0.0   \n",
       "3      Do not forget to get some sunshine safely if y...       0.0   0.0   \n",
       "4      Abstract Background Humans have dramatically c...       0.0   0.0   \n",
       "...                                                  ...       ...   ...   \n",
       "10567  ‚òë Day: 273 Outifit: Thor (Gold Foil) Back Blin...       0.0   0.0   \n",
       "10568  In order to study the various trends and patte...       0.0   0.0   \n",
       "10569  This brand has not yet registered with Influen...       0.0   0.0   \n",
       "10570  here is my regime get rid of any acne scars an...       1.0   0.0   \n",
       "10571  It's true, I used to work as a caddie at Trump...       0.0   0.0   \n",
       "\n",
       "       make-up  other  index  \n",
       "0          0.0    1.0      0  \n",
       "1          0.0    1.0      1  \n",
       "2          1.0    0.0      2  \n",
       "3          0.0    1.0      3  \n",
       "4          0.0    1.0      4  \n",
       "...        ...    ...    ...  \n",
       "10567      0.0    1.0  13209  \n",
       "10568      0.0    1.0  13210  \n",
       "10569      0.0    1.0  13212  \n",
       "10570      0.0    0.0  13213  \n",
       "10571      0.0    1.0  13214  \n",
       "\n",
       "[10572 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>skincare</th>\n      <th>hair</th>\n      <th>make-up</th>\n      <th>other</th>\n      <th>index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Skeeterbytes wrote: Bassam Guy wrote: I don't ...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Refueling before the long week üçµ - You‚Äôre mean...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Just in case no one has told you today YOU ARE...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Do not forget to get some sunshine safely if y...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Abstract Background Humans have dramatically c...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10567</th>\n      <td>‚òë Day: 273 Outifit: Thor (Gold Foil) Back Blin...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>13209</td>\n    </tr>\n    <tr>\n      <th>10568</th>\n      <td>In order to study the various trends and patte...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>13210</td>\n    </tr>\n    <tr>\n      <th>10569</th>\n      <td>This brand has not yet registered with Influen...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>13212</td>\n    </tr>\n    <tr>\n      <th>10570</th>\n      <td>here is my regime get rid of any acne scars an...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>13213</td>\n    </tr>\n    <tr>\n      <th>10571</th>\n      <td>It's true, I used to work as a caddie at Trump...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>13214</td>\n    </tr>\n  </tbody>\n</table>\n<p>10572 rows √ó 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df_train = df_train.drop(['Unnamed: 0'], axis = 1)\n",
    "df_train"
   ]
  },
  {
   "source": [
    "## see disparite"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "skincare  :  1303\nhair  :  1272\nmake-up  :  1907\nother  :  6667\n"
     ]
    }
   ],
   "source": [
    "for val in array_col:\n",
    "    len_result = len(df_train[df_train[val] == 1])\n",
    "    print(val, \" : \" ,len_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing one text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Onetext = df_train['text'].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to remove emoji.\n",
    "def emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "                           \n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Just in case no one has told you today YOU ARE STRONG BEAUTIFUL I LOVE YOU  Por si no te lo han dicho hoy TU ERES FUERTE BONITA Y TE AMO  Wicked Gel Liner coupon code BIRDY Hydrating spray code BIRDY pearl radiance primer foundation in almond shape tape contour concealer hoola contour stick bronzer powder translucent powder rose gold highlight code BIRDY whirl lip liner Nude potion BIRDY'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "emoji(Onetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "OnetextLower = Onetext.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizeSingletext = word_tokenize(OnetextLower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizeSingleText = list(map(lemmatizer.lemmatize, tokenizeSingletext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_punctuation = [':', '(', ')', '/', '|', ',', ']', ';',\n",
    "                    '.', '*', '#', '\"', '&', '~', '``',\n",
    "                    '-', '_', '\\\\', '@','?','!','\\'', '[', '<', '>', \"‚Äô\"]\n",
    "TextCleanPonctuation = [word for word in lemmatizeSingleText if word not in stop_punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = stopwords.words('english')\n",
    "TextCleanPonctuationWord = [word for word in TextCleanPonctuation if word not in stoplist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('case', 'NN'),\n",
       " ('one', 'CD'),\n",
       " ('ha', 'NN'),\n",
       " ('told', 'VBD'),\n",
       " ('today', 'NN'),\n",
       " ('strong', 'JJ'),\n",
       " ('beautiful', 'JJ'),\n",
       " ('love', 'NN'),\n",
       " ('üåπ', 'NNP'),\n",
       " ('por', 'NN'),\n",
       " ('si', 'NN'),\n",
       " ('te', 'NN'),\n",
       " ('lo', 'NN'),\n",
       " ('han', 'NN'),\n",
       " ('dicho', 'NN'),\n",
       " ('hoy', 'NN'),\n",
       " ('tu', 'NN'),\n",
       " ('eres', 'VBZ'),\n",
       " ('fuerte', 'JJ'),\n",
       " ('bonita', 'NN'),\n",
       " ('te', 'NN'),\n",
       " ('amo', 'NN'),\n",
       " ('üåπ', 'NNP'),\n",
       " ('wicked', 'VBD'),\n",
       " ('gel', 'JJ'),\n",
       " ('liner', 'NN'),\n",
       " ('coupon', 'NN'),\n",
       " ('code', 'NN'),\n",
       " ('birdy', 'IN'),\n",
       " ('hydrating', 'VBG'),\n",
       " ('spray', 'NN'),\n",
       " ('code', 'NN'),\n",
       " ('birdy', 'NN'),\n",
       " ('pearl', 'NN'),\n",
       " ('radiance', 'NN'),\n",
       " ('primer', 'NN'),\n",
       " ('foundation', 'NN'),\n",
       " ('almond', 'NN'),\n",
       " ('shape', 'NN'),\n",
       " ('tape', 'NN'),\n",
       " ('contour', 'VBP'),\n",
       " ('concealer', 'NN'),\n",
       " ('hoola', 'NN'),\n",
       " ('contour', 'NN'),\n",
       " ('stick', 'NN'),\n",
       " ('bronzer', 'NN'),\n",
       " ('powder', 'NN'),\n",
       " ('translucent', 'NN'),\n",
       " ('powder', 'NN'),\n",
       " ('rose', 'VBD'),\n",
       " ('gold', 'JJ'),\n",
       " ('highlight', 'NN'),\n",
       " ('code', 'NN'),\n",
       " ('birdy', 'IN'),\n",
       " ('whirl', 'NN'),\n",
       " ('lip', 'NN'),\n",
       " ('liner', 'NN'),\n",
       " ('nude', 'JJ'),\n",
       " ('potion', 'NN'),\n",
       " ('birdy', 'NN')]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "#from textblob import TextBlob\n",
    "#regroupe_var = ' '.join(TextCleanPonctuationWord)\n",
    "\n",
    "#result = TextBlob(regroupe_var)\n",
    "#result.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle  \n",
    "\n",
    "with open('Emoji_Dict.p', 'rb') as fp:\n",
    "    Emoji_Dict = pickle.load(fp)\n",
    "Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n",
    "\n",
    "\n",
    "\n",
    "def convert_emojis_to_word(text):\n",
    "    for emot in Emoji_Dict:\n",
    "        text = re.sub(r'('+emot+')', \"_\".join(Emoji_Dict[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Function to remove emoji.\n",
    "def del_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "                           \n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def text_clean_ponctuation(text):\n",
    "    stop_punctuation = [':', '(', ')', '/', '|', ',', ']', ';',\n",
    "                    '.', '*', '#', '\"', '&', '~', '``',\n",
    "                    '-', '_', '\\\\', '@','?','!','\\'', '[', '<', '>', '¬£', '$', '‚Äù', \"\\u2063\", \"‚Ä¢\", \"'s\", \"‚Äú\"]\n",
    "    stoplist = stopwords.words('english')\n",
    "\n",
    "    TextCleanPonctuation = [word for word in text if word not in stop_punctuation and word not in stoplist]\n",
    "    return TextCleanPonctuation\n",
    "\n",
    "def regroupe_text(list_word):\n",
    "    return ' '.join(list_word)\n",
    "\n",
    "def count_text(text):\n",
    "    cv=CountVectorizer()\n",
    "    text_count = word_count_vector=cv.fit_transform(text)\n",
    "    return text_count\n",
    "\n",
    "def preprocessing(singleText):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    singleText = del_emoji(singleText)\n",
    "    LowerSingleText = singleText.lower()\n",
    "    tokenizeSingletext = word_tokenize(LowerSingleText)\n",
    "    lemmatizeSingleText = list(map(lemmatizer.lemmatize, tokenizeSingletext))\n",
    "    TextCleanPonctuation = text_clean_ponctuation(lemmatizeSingleText)\n",
    "    # text_count_result = count_text( )\n",
    "\n",
    "    return TextCleanPonctuation\n",
    "\n",
    "def clean_text(text):\n",
    "    text_preprocessed = preprocessing(text)\n",
    "    return  regroupe_text(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"skeeterbytes wrote bassam guy wrote n't anything longer 60mm except 99 plastic fantastic '' 40-150. 've never really tried avian photography would like get decent shot unlike opportunity arises used c-af+tr retrospect since turkey lawyer refused fly despite plea would s-af done better mf peaking magnify exposed lawyer dressed dark suit bright cloudy sky meteorologist say smoke remnant ca northern va wa +2 +2.5 ev much encouraged chroma 'm great judging distance would say wa 35+ meter away pretty intense purple fringing lightroom ha effective one-click remover 'm sure software ha hard control know panasonic 7-14 4 never use c-af+tr preferring standard c-af s-af instance s-af fine attorney n't going anywhere pay bill get s-af+magnify allow tweak focus plus want stop lens bit sharpen except 7-14 nothing else f2.8 5.6 seems stopped 'll watch tried magnify wa shaky long fl 'll try lower scale factor also raise shutter speed freeze flapping 're iso200 good lens within realm 40-150 pro league better 10x price birding test petapixel fstoppers rated e-m5 iii pretty well c-af+tr n't use often location 5 minute walk home perhaps 'll try keep experimenting rick thanks helpful reply\""
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "resultOneText = clean_text(df_train[\"text\"].iloc[0])\n",
    "resultOneText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On applique cela pour toute la base de donn√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0        skeeterbytes wrote bassam guy wrote n't anythi...\n",
       "1        refueling long week ‚Äô meant drink prefer match...\n",
       "2        case one ha told today strong beautiful love p...\n",
       "3        forget get sunshine safely head link bio see s...\n",
       "4        abstract background human dramatically changed...\n",
       "                               ...                        \n",
       "10567    day 273 outifit thor gold foil back bling eter...\n",
       "10568    order study various trend pattern prevailing c...\n",
       "10569    brand ha yet registered influenster work brand...\n",
       "10570    regime get rid acne scar get way smoother heal...\n",
       "10571    true used work caddie trump national nj donny ...\n",
       "Name: clean_content, Length: 10572, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df_train[\"clean_content\"] = df_train.text.apply(clean_text)\n",
    "df_train[\"clean_content\"]"
   ]
  },
  {
   "source": [
    "# Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0        skeeterbytes wrote bassam guy wrote n't anythi...\n",
       "1        refueling long week ‚Äô meant drink prefer match...\n",
       "2        case one ha told today strong beautiful love p...\n",
       "3        forget get sunshine safely head link bio see s...\n",
       "4        abstract background human dramatically changed...\n",
       "                               ...                        \n",
       "10567    day 273 outifit thor gold foil back bling eter...\n",
       "10568    order study various trend pattern prevailing c...\n",
       "10569    brand ha yet registered influenster work brand...\n",
       "10570    regime get rid acne scar get way smoother heal...\n",
       "10571    true used work caddie trump national nj donny ...\n",
       "Name: clean_content, Length: 10572, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "df_train_clean = df_train.drop([\"text\"], axis = 1)\n",
    "X = df_train['clean_content']\n",
    "Y = df_train.drop(['clean_content', 'index', 'text'], axis = 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1488    ‚Äô mad bonkers completely head ‚Äô tell secret be...\n",
       "3072    website ‚Äô dashboard welcoming website ‚Äô home p...\n",
       "7753    yomazu city cold night wa aptly named biting w...\n",
       "7304    sell-side analyst projecting nokia corporation...\n",
       "1059    moresimply naked beauty maker one amazon ‚Äô mos...\n",
       "                              ...                        \n",
       "456     work well defiently need work skin hand mit li...\n",
       "6017    grey rapped fair knuckle glass jewelry display...\n",
       "709     whenever wellness middle east mentioned togeth...\n",
       "8366    one like photo banticicphotography hair salona...\n",
       "1146    friday teva pharmaceutical industry limited ny...\n",
       "Name: clean_content, Length: 8457, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=4)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   text  skincare  hair  \\\n",
       "1488  ‚ÄúYou‚Äôre mad, bonkers, completely off your head...       1.0   0.0   \n",
       "3072  Your website‚Äôs dashboard should be as welcomin...       0.0   0.0   \n",
       "7753  Yomazu the City of Cold Night It was aptly nam...       0.0   0.0   \n",
       "7304  Sell-side analysts are projecting that Nokia C...       0.0   0.0   \n",
       "1059  MoreSimply Naked Beauty, the makers of one of ...       0.0   0.0   \n",
       "...                                                 ...       ...   ...   \n",
       "456   Works well but defiently need to work it into ...       0.0   0.0   \n",
       "6017  Grey rapped his fair knuckles against the glas...       0.0   0.0   \n",
       "709   Whenever wellness and the Middle East are ment...       0.0   0.0   \n",
       "8366  üëºüèªWhich one you like more üëºüèª or Photo banticic...       0.0   1.0   \n",
       "1146  On Friday the Teva Pharmaceutical Industries L...       0.0   0.0   \n",
       "\n",
       "      make-up  other  \n",
       "1488      1.0    0.0  \n",
       "3072      0.0    1.0  \n",
       "7753      0.0    1.0  \n",
       "7304      0.0    1.0  \n",
       "1059      1.0    0.0  \n",
       "...       ...    ...  \n",
       "456       1.0    0.0  \n",
       "6017      0.0    1.0  \n",
       "709       0.0    1.0  \n",
       "8366      0.0    1.0  \n",
       "1146      0.0    1.0  \n",
       "\n",
       "[8457 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>skincare</th>\n      <th>hair</th>\n      <th>make-up</th>\n      <th>other</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1488</th>\n      <td>‚ÄúYou‚Äôre mad, bonkers, completely off your head...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3072</th>\n      <td>Your website‚Äôs dashboard should be as welcomin...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>7753</th>\n      <td>Yomazu the City of Cold Night It was aptly nam...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>7304</th>\n      <td>Sell-side analysts are projecting that Nokia C...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1059</th>\n      <td>MoreSimply Naked Beauty, the makers of one of ...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>456</th>\n      <td>Works well but defiently need to work it into ...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6017</th>\n      <td>Grey rapped his fair knuckles against the glas...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>709</th>\n      <td>Whenever wellness and the Middle East are ment...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8366</th>\n      <td>üëºüèªWhich one you like more üëºüèª or Photo banticic...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1146</th>\n      <td>On Friday the Teva Pharmaceutical Industries L...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>8457 rows √ó 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "source": [
    "## vectorization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Count"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "cv=CountVectorizer()\n",
    "cv.fit(x_train)\n",
    "x_train_count = cv.fit_transform(x_train).astype('int32').toarray()\n",
    "x_test_count = cv.transform(x_test).astype('int32').toarray()\n",
    "\n",
    "type(x_train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for val in x_train_count[0]:\n",
    "    if type(val) != type(x_train_count[0][0]):\n",
    "        print(type(val))"
   ]
  },
  {
   "source": [
    "# Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## machine learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "265/265 [==============================] - 20s 76ms/step - loss: 0.2593 - f1_m: 0.7855\n",
      "Epoch 2/5\n",
      "265/265 [==============================] - 20s 75ms/step - loss: 0.1108 - f1_m: 0.9221\n",
      "Epoch 3/5\n",
      "265/265 [==============================] - 20s 75ms/step - loss: 0.0638 - f1_m: 0.9560\n",
      "Epoch 4/5\n",
      "265/265 [==============================] - 20s 76ms/step - loss: 0.0494 - f1_m: 0.9649\n",
      "Epoch 5/5\n",
      "265/265 [==============================] - 20s 74ms/step - loss: 0.0419 - f1_m: 0.9675\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2327fa24d30>"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_shape=(70574,), activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = [f1_m])\n",
    "\n",
    "model.fit(x_train_count, y_train, epochs=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "67/67 [==============================] - 1s 21ms/step - loss: 0.2657 - f1_m: 0.8685\n"
     ]
    }
   ],
   "source": [
    "evaluate = model.evaluate(x_test_count, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualisation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "score = clf.predict_proba(feat_test)\n",
    "score\n",
    "#fpr, tpr, th = roc_curve(y_list_test, score[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "aucf = auc(fpr, tpr)\n",
    "ax.plot(fpr, tpr, label='auc=%1.5f' % aucf)\n",
    "ax.set_title('Courbe ROC - classifieur de sentiments')\n",
    "ax.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}